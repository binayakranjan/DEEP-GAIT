{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE523.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binayakranjan/DEEP-GAIT/blob/master/CSE523.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex3qQJGbaAxb",
        "colab_type": "code",
        "outputId": "995f014b-5ab9-4972-d8a9-2d792364f5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aupcPLfnaCw2",
        "colab_type": "code",
        "outputId": "b9e81085-db17-4616-e172-52e8279ceaa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtlSJWosx06m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "3b1a8c18-4976-4e9c-fad9-3f74242969b9"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1014-practice-questions.pdf\n",
            " 191021-serializability.mov\n",
            "'5 lb. Book of GRE Practice Problems - Manhattan Prep.pdf'\n",
            " AIMCAT12.zip\n",
            " Barrons_New_GRE_19th_ed.pdf\n",
            " Binayak_Additional_info.docx\n",
            "'BinayakDasCv - binayakranjan das.pdf'\n",
            " BinayakDasCv.pdf\n",
            " BinayakRanjanDas.pdf\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "\u001b[01;34m'Google Buzz'\u001b[0m/\n",
            "'GRE Math Bible.pdf NOVA.pdf'\n",
            "'Lic Receipt.jpg'\n",
            " MAGOOSH+589+QUANT+PRACTICE+QUESTIONS.pdf\n",
            " main.pdf\n",
            "'Monthly Expenditure.gsheet'\n",
            "'mu sigma id.gsheet'\n",
            "\u001b[01;34m'New Folder'\u001b[0m/\n",
            "'passport .pdf'\n",
            "'Photo album.gslides'\n",
            "'photo immigration .pdf'\n",
            "'Resume (1).gdoc'\n",
            "'Resume (2).gdoc'\n",
            "'Resume (3).gdoc'\n",
            "'Resume (4).gdoc'\n",
            "'Resume (5).gdoc'\n",
            "'Resume (6).gdoc'\n",
            "'Resume (7).gdoc'\n",
            " Resume.gdoc\n",
            " run.pkl\n",
            "'sapient test results.gsheet'\n",
            " Scanned_20190820-1133.pdf\n",
            " \u001b[01;34mSHIVSAMUDRAM\u001b[0m/\n",
            " stonyVisa.pdf\n",
            " store_data.csv\n",
            " \u001b[01;34mSW-DESIGN\u001b[0m/\n",
            " \u001b[01;34mtest\u001b[0m/\n",
            " \u001b[01;34mtest2\u001b[0m/\n",
            " Test_data.csv\n",
            " \u001b[01;34mtrain\u001b[0m/\n",
            " Training_data.csv\n",
            "'Untitled document.gdoc'\n",
            "'Untitled form [Form].gform'\n",
            "'Untitled form.gsheet'\n",
            "'Untitled spreadsheet.gsheet'\n",
            " walk.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Ovu4h1vWrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "613c1050-4535-4cb8-a3f4-753146873d0b"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCwE8E-Bv7t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data_path, batch_size):\n",
        "\n",
        "    data = np.load(data_path, allow_pickle = True)\n",
        "    num_class = len(data)\n",
        "\n",
        "    pairs = []\n",
        "    pair_idx = []\n",
        "    labels = []\n",
        "\n",
        "    for user_idx, user_data in enumerate(data):\n",
        "\n",
        "        user_label = np.zeros(batch_size)\n",
        "        user_label[:len(user_label)//2] = 1\n",
        "\n",
        "        num_data = len(user_data)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "\n",
        "            batch = []\n",
        "            batch_idx = [user_idx]\n",
        "            random_idx = random.randint(0, num_data-1)\n",
        "            batch.append(user_data[random_idx])\n",
        "\n",
        "            if idx < len(user_label)//2:\n",
        "                random_idx = random.randint(0, num_data-1)\n",
        "                batch.append(user_data[random_idx])\n",
        "                batch_idx.append(user_idx)\n",
        "            else:\n",
        "                random_user = (user_idx + random.randint(1,num_class-1)) % num_class\n",
        "                random_user_data = data[random_user]\n",
        "\n",
        "                random_idx = random.randint(0, len(random_user_data)-1)\n",
        "                batch.append(random_user_data[random_idx])\n",
        "                batch_idx.append(random_user)\n",
        "\n",
        "            pairs.append(batch)\n",
        "            pair_idx.append(batch_idx)\n",
        "        labels.append(user_label)\n",
        "\n",
        "\n",
        "    return np.array(pairs), np.array(pair_idx), np.array(labels).ravel().reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ow5e_l9wCD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_data(data, index, label, ratio):\n",
        "\n",
        "    data, index, label = shuffle(data, index, label)\n",
        "\n",
        "    train_data, train_index, train_label = data[:int(len(data)*ratio)], index[:int(len(data)*ratio)], label[:int(len(data)*ratio)]\n",
        "    test_data, test_index, test_label = data[int(len(data)*ratio):], index[int(len(data)*ratio):], label[int(len(data)*ratio):]\n",
        "\n",
        "    return train_data, train_index, train_label, test_data, test_index, test_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A4JTPucwE9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(data, index, label, ratio):\n",
        "\n",
        "    train_data, train_index, train_label, test_data, test_index, test_label = shuffle_data(data, index, label, ratio)\n",
        "\n",
        "    while np.any(np.unique(train_index) != np.unique(index)):\n",
        "\n",
        "        train_data, train_index, train_label, test_data, test_index, test_label = shuffle_data(data, index, label, ratio)\n",
        "\n",
        "    return train_data, train_index, train_label, test_data, test_index, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRRU715FwLKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "The output of Siamese network would be similarity score, which indicates a pair of gait cycle are from the same user or not (binary classification)\n",
        "'''\n",
        "def Layer(X, num_output, initializer, keep_prob, W_name, b_name):\n",
        "\n",
        "    _, num_feature = X.shape\n",
        "\n",
        "    W = tf.get_variable(W_name, shape = [num_feature, num_output], dtype = tf.float32, initializer = initializer)\n",
        "    b = tf.Variable(tf.random_normal([num_output]), name = b_name)\n",
        "    L = tf.matmul(X, W) + b\n",
        "    L = tf.nn.relu(L)\n",
        "    L = tf.nn.dropout(L, keep_prob = keep_prob)\n",
        "\n",
        "    return L\n",
        "\n",
        "def siamese(input_data, keep_prob, reuse = False):\n",
        "\n",
        "    l1_dim = 2000\n",
        "    l2_dim = 3000\n",
        "    l3_dim = 3000\n",
        "\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    with tf.variable_scope('Layer1', reuse = reuse) as scope:\n",
        "        model = Layer(input_data, l1_dim, initializer, keep_prob, 'W1', 'b1')\n",
        "\n",
        "    with tf.variable_scope('Layer2', reuse = reuse) as scope:\n",
        "        model = Layer(model, l2_dim, initializer, keep_prob, 'W2', 'b2')\n",
        "\n",
        "    with tf.variable_scope('Layer3', reuse = reuse) as scope:\n",
        "        model = Layer(model, l3_dim, initializer, keep_prob, 'W3', 'b3')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwS9KhUUvL9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c876a01a-7441-428e-db67-8dd73110b23e"
      },
      "source": [
        "#walk_data_path = 'data/walk/filtered_interpolation.pkl'\n",
        "walk_data_path = 'run.pkl'\n",
        "batch_size = 80\n",
        "num_iter = 500\n",
        "initial_learning_rate = 10**(-4)\n",
        "\n",
        "pairs, pair_idx, labels = get_data(walk_data_path, batch_size)\n",
        "num_pairs, pair_size, cycle_length, num_feature = pairs.shape\n",
        "print(labels.shape)\n",
        "train_data, train_index, train_label, test_data, test_index, test_label = train_test_split(pairs, pair_idx, labels, 0.7)\n",
        "print(train_data.shape, train_index.shape, train_label.shape)\n",
        "\n",
        "left = tf.placeholder(tf.float32, shape = [None, cycle_length, num_feature], name = 'left')\n",
        "right = tf.placeholder(tf.float32, shape = [None, cycle_length, num_feature], name = 'right')\n",
        "\n",
        "new_left = tf.reshape(left, [-1, cycle_length*num_feature])\n",
        "new_right = tf.reshape(right, [-1, cycle_length*num_feature])\n",
        "\n",
        "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "\n",
        "left_model = siamese(new_left, keep_prob, False)\n",
        "right_model = siamese(new_right, keep_prob, True)\n",
        "\n",
        "with tf.variable_scope('Difference'):\n",
        "    difference = tf.math.abs(left_model - right_model)\n",
        "\n",
        "with tf.variable_scope('Dense'):\n",
        "\n",
        "    W = tf.get_variable('W', shape = [difference.shape[-1], 1], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
        "    b = tf.Variable(tf.random_normal([1]), name = 'b')\n",
        "    L = tf.matmul(difference, W) + b\n",
        "\n",
        "with tf.name_scope('Training'):\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = L, labels = Y))\n",
        "\n",
        "    global_step = tf.Variable(0)\n",
        "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, 100, 0.9)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "    similarity_score = tf.nn.sigmoid(L)\n",
        "    #tf.print(similarity_score)\n",
        "    #tf.print(Y)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(similarity_score), tf.round(Y)), tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_iter):\n",
        "\n",
        "        c, a, _ = sess.run([cost, accuracy, optimizer], feed_dict = {left: train_data[:, 0, :, :], right: train_data[:, 1, :, :], Y: train_label, keep_prob: 0.7})\n",
        "        print('Cost: {}, Accuracy: {}'.format(c, a))\n",
        "\n",
        "    acc = sess.run(accuracy, feed_dict = {left: test_data[:, 0, :, :], right: test_data[:, 1, :, :], Y: test_label, keep_prob: 1.0})\n",
        "    print(acc)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 1)\n",
            "(2800, 2, 200, 6) (2800, 2) (2800, 1)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-a3448ab9cda2>:13: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Cost: 2.598468542098999, Accuracy: 0.48892858624458313\n",
            "Cost: 15.979519844055176, Accuracy: 0.508571445941925\n",
            "Cost: 10.502933502197266, Accuracy: 0.508571445941925\n",
            "Cost: 1.9829663038253784, Accuracy: 0.5274999737739563\n",
            "Cost: 8.312134742736816, Accuracy: 0.49142858386039734\n",
            "Cost: 9.535563468933105, Accuracy: 0.49142858386039734\n",
            "Cost: 6.6169753074646, Accuracy: 0.49214285612106323\n",
            "Cost: 2.2298014163970947, Accuracy: 0.5371428728103638\n",
            "Cost: 2.666285276412964, Accuracy: 0.5378571152687073\n",
            "Cost: 4.68552827835083, Accuracy: 0.512499988079071\n",
            "Cost: 5.0570759773254395, Accuracy: 0.5092856884002686\n",
            "Cost: 4.110902309417725, Accuracy: 0.5092856884002686\n",
            "Cost: 2.3397252559661865, Accuracy: 0.5346428751945496\n",
            "Cost: 1.2921974658966064, Accuracy: 0.5824999809265137\n",
            "Cost: 2.0770339965820312, Accuracy: 0.5453571677207947\n",
            "Cost: 2.8847081661224365, Accuracy: 0.5182142853736877\n",
            "Cost: 2.889735221862793, Accuracy: 0.5139285922050476\n",
            "Cost: 2.1639175415039062, Accuracy: 0.5389285683631897\n",
            "Cost: 1.3361282348632812, Accuracy: 0.583214282989502\n",
            "Cost: 1.0195664167404175, Accuracy: 0.5960714221000671\n",
            "Cost: 1.4527747631072998, Accuracy: 0.5642856955528259\n",
            "Cost: 1.9121674299240112, Accuracy: 0.5353571176528931\n",
            "Cost: 1.8269635438919067, Accuracy: 0.5335714221000671\n",
            "Cost: 1.4904627799987793, Accuracy: 0.5635714530944824\n",
            "Cost: 1.0550990104675293, Accuracy: 0.5924999713897705\n",
            "Cost: 0.927321195602417, Accuracy: 0.6153571605682373\n",
            "Cost: 1.0357131958007812, Accuracy: 0.6096428632736206\n",
            "Cost: 1.2573469877243042, Accuracy: 0.5892857313156128\n",
            "Cost: 1.332162618637085, Accuracy: 0.571071445941925\n",
            "Cost: 1.0661954879760742, Accuracy: 0.6085714101791382\n",
            "Cost: 0.8615496158599854, Accuracy: 0.6364285945892334\n",
            "Cost: 0.7922834157943726, Accuracy: 0.6253571510314941\n",
            "Cost: 0.8872178196907043, Accuracy: 0.6228571534156799\n",
            "Cost: 0.9627717137336731, Accuracy: 0.616428554058075\n",
            "Cost: 0.9927874803543091, Accuracy: 0.5992857217788696\n",
            "Cost: 0.9029816389083862, Accuracy: 0.6260714530944824\n",
            "Cost: 0.7916250824928284, Accuracy: 0.6414285898208618\n",
            "Cost: 0.7584360241889954, Accuracy: 0.6517857313156128\n",
            "Cost: 0.7475034594535828, Accuracy: 0.6628571152687073\n",
            "Cost: 0.8023489117622375, Accuracy: 0.6600000262260437\n",
            "Cost: 0.8496118783950806, Accuracy: 0.647857129573822\n",
            "Cost: 0.8259512782096863, Accuracy: 0.6560714244842529\n",
            "Cost: 0.7423997521400452, Accuracy: 0.6664285659790039\n",
            "Cost: 0.6840522885322571, Accuracy: 0.6778571605682373\n",
            "Cost: 0.7229368686676025, Accuracy: 0.6603571176528931\n",
            "Cost: 0.7556499242782593, Accuracy: 0.6589285731315613\n",
            "Cost: 0.751869261264801, Accuracy: 0.6696428656578064\n",
            "Cost: 0.7697516679763794, Accuracy: 0.6560714244842529\n",
            "Cost: 0.6893408298492432, Accuracy: 0.681071400642395\n",
            "Cost: 0.6874814629554749, Accuracy: 0.6889285445213318\n",
            "Cost: 0.6771950721740723, Accuracy: 0.693928599357605\n",
            "Cost: 0.6902081966400146, Accuracy: 0.6807143092155457\n",
            "Cost: 0.7048084735870361, Accuracy: 0.6828571557998657\n",
            "Cost: 0.693092942237854, Accuracy: 0.6892856955528259\n",
            "Cost: 0.6739510893821716, Accuracy: 0.7017857432365417\n",
            "Cost: 0.6426566243171692, Accuracy: 0.7071428298950195\n",
            "Cost: 0.6448171734809875, Accuracy: 0.693928599357605\n",
            "Cost: 0.6054570078849792, Accuracy: 0.7089285850524902\n",
            "Cost: 0.6484065651893616, Accuracy: 0.7003571391105652\n",
            "Cost: 0.6299260258674622, Accuracy: 0.7049999833106995\n",
            "Cost: 0.6352965831756592, Accuracy: 0.7053571343421936\n",
            "Cost: 0.6002500057220459, Accuracy: 0.7200000286102295\n",
            "Cost: 0.615345299243927, Accuracy: 0.7214285731315613\n",
            "Cost: 0.6078276634216309, Accuracy: 0.7196428775787354\n",
            "Cost: 0.6395598649978638, Accuracy: 0.7035714387893677\n",
            "Cost: 0.5972756743431091, Accuracy: 0.7285714149475098\n",
            "Cost: 0.640410304069519, Accuracy: 0.7132142782211304\n",
            "Cost: 0.5919377207756042, Accuracy: 0.7121428847312927\n",
            "Cost: 0.6108863353729248, Accuracy: 0.7074999809265137\n",
            "Cost: 0.5912089347839355, Accuracy: 0.7200000286102295\n",
            "Cost: 0.576833188533783, Accuracy: 0.727142870426178\n",
            "Cost: 0.5863857865333557, Accuracy: 0.7210714221000671\n",
            "Cost: 0.5742999911308289, Accuracy: 0.7357142567634583\n",
            "Cost: 0.5770002007484436, Accuracy: 0.7403571605682373\n",
            "Cost: 0.5899275541305542, Accuracy: 0.7214285731315613\n",
            "Cost: 0.5766358971595764, Accuracy: 0.7339285612106323\n",
            "Cost: 0.5540583729743958, Accuracy: 0.7439285516738892\n",
            "Cost: 0.5828471779823303, Accuracy: 0.739642858505249\n",
            "Cost: 0.5654609203338623, Accuracy: 0.7425000071525574\n",
            "Cost: 0.556869387626648, Accuracy: 0.7264285683631897\n",
            "Cost: 0.5618736743927002, Accuracy: 0.7285714149475098\n",
            "Cost: 0.5626131892204285, Accuracy: 0.7464285492897034\n",
            "Cost: 0.5577729344367981, Accuracy: 0.7478571534156799\n",
            "Cost: 0.5419951677322388, Accuracy: 0.7432143092155457\n",
            "Cost: 0.5362634062767029, Accuracy: 0.7428571581840515\n",
            "Cost: 0.5585551857948303, Accuracy: 0.7457143068313599\n",
            "Cost: 0.5292062163352966, Accuracy: 0.7489285469055176\n",
            "Cost: 0.5368156433105469, Accuracy: 0.7517856955528259\n",
            "Cost: 0.524230420589447, Accuracy: 0.7482143044471741\n",
            "Cost: 0.5310416221618652, Accuracy: 0.7546428442001343\n",
            "Cost: 0.5294240117073059, Accuracy: 0.7478571534156799\n",
            "Cost: 0.5021213889122009, Accuracy: 0.7714285850524902\n",
            "Cost: 0.5278334021568298, Accuracy: 0.756428599357605\n",
            "Cost: 0.5017279982566833, Accuracy: 0.7649999856948853\n",
            "Cost: 0.5184153318405151, Accuracy: 0.7589285969734192\n",
            "Cost: 0.5134572982788086, Accuracy: 0.7671428322792053\n",
            "Cost: 0.5085200667381287, Accuracy: 0.768928587436676\n",
            "Cost: 0.5069062113761902, Accuracy: 0.7696428298950195\n",
            "Cost: 0.5364507436752319, Accuracy: 0.7557142972946167\n",
            "Cost: 0.5168229341506958, Accuracy: 0.760357141494751\n",
            "Cost: 0.5027951598167419, Accuracy: 0.7703571319580078\n",
            "Cost: 0.4938489496707916, Accuracy: 0.7753571271896362\n",
            "Cost: 0.4838016927242279, Accuracy: 0.768928587436676\n",
            "Cost: 0.49045297503471375, Accuracy: 0.7717857360839844\n",
            "Cost: 0.5187544226646423, Accuracy: 0.7610714435577393\n",
            "Cost: 0.46744322776794434, Accuracy: 0.7749999761581421\n",
            "Cost: 0.4815404415130615, Accuracy: 0.7853571176528931\n",
            "Cost: 0.4765265882015228, Accuracy: 0.7796428799629211\n",
            "Cost: 0.4717886745929718, Accuracy: 0.7817857265472412\n",
            "Cost: 0.4708692133426666, Accuracy: 0.7875000238418579\n",
            "Cost: 0.46848711371421814, Accuracy: 0.7892857193946838\n",
            "Cost: 0.44440317153930664, Accuracy: 0.7889285683631897\n",
            "Cost: 0.4671800434589386, Accuracy: 0.7789285778999329\n",
            "Cost: 0.47890931367874146, Accuracy: 0.7742857336997986\n",
            "Cost: 0.47384583950042725, Accuracy: 0.779285728931427\n",
            "Cost: 0.47679731249809265, Accuracy: 0.7857142686843872\n",
            "Cost: 0.4779106676578522, Accuracy: 0.7850000262260437\n",
            "Cost: 0.45222634077072144, Accuracy: 0.793571412563324\n",
            "Cost: 0.4405933916568756, Accuracy: 0.7910714149475098\n",
            "Cost: 0.45566847920417786, Accuracy: 0.7882142663002014\n",
            "Cost: 0.44745656847953796, Accuracy: 0.7960714101791382\n",
            "Cost: 0.47531893849372864, Accuracy: 0.7892857193946838\n",
            "Cost: 0.4557458162307739, Accuracy: 0.8010714054107666\n",
            "Cost: 0.44447261095046997, Accuracy: 0.7950000166893005\n",
            "Cost: 0.42627161741256714, Accuracy: 0.7975000143051147\n",
            "Cost: 0.4487546682357788, Accuracy: 0.7953571677207947\n",
            "Cost: 0.43474164605140686, Accuracy: 0.8096428513526917\n",
            "Cost: 0.42967748641967773, Accuracy: 0.7975000143051147\n",
            "Cost: 0.4395139813423157, Accuracy: 0.7982142567634583\n",
            "Cost: 0.43096187710762024, Accuracy: 0.8092857003211975\n",
            "Cost: 0.4206574261188507, Accuracy: 0.8071428537368774\n",
            "Cost: 0.4228200316429138, Accuracy: 0.8064285516738892\n",
            "Cost: 0.42880523204803467, Accuracy: 0.7996428608894348\n",
            "Cost: 0.43082571029663086, Accuracy: 0.8064285516738892\n",
            "Cost: 0.4282938838005066, Accuracy: 0.8053571581840515\n",
            "Cost: 0.4038292467594147, Accuracy: 0.8167856931686401\n",
            "Cost: 0.4102795422077179, Accuracy: 0.8107143044471741\n",
            "Cost: 0.413341760635376, Accuracy: 0.808571457862854\n",
            "Cost: 0.39927831292152405, Accuracy: 0.8192856907844543\n",
            "Cost: 0.41114580631256104, Accuracy: 0.8178571462631226\n",
            "Cost: 0.42597436904907227, Accuracy: 0.8082143068313599\n",
            "Cost: 0.41187363862991333, Accuracy: 0.8196428418159485\n",
            "Cost: 0.4171510636806488, Accuracy: 0.8117856979370117\n",
            "Cost: 0.41127485036849976, Accuracy: 0.8185714483261108\n",
            "Cost: 0.4046265482902527, Accuracy: 0.8139285445213318\n",
            "Cost: 0.38895145058631897, Accuracy: 0.8207142949104309\n",
            "Cost: 0.3931524157524109, Accuracy: 0.824999988079071\n",
            "Cost: 0.39059317111968994, Accuracy: 0.8224999904632568\n",
            "Cost: 0.3898022770881653, Accuracy: 0.8235714435577393\n",
            "Cost: 0.3879380524158478, Accuracy: 0.8321428298950195\n",
            "Cost: 0.38896191120147705, Accuracy: 0.8235714435577393\n",
            "Cost: 0.37558498978614807, Accuracy: 0.8360714316368103\n",
            "Cost: 0.39351919293403625, Accuracy: 0.829285740852356\n",
            "Cost: 0.3876413106918335, Accuracy: 0.8196428418159485\n",
            "Cost: 0.37110841274261475, Accuracy: 0.8385714292526245\n",
            "Cost: 0.3787705898284912, Accuracy: 0.8421428799629211\n",
            "Cost: 0.36557137966156006, Accuracy: 0.8324999809265137\n",
            "Cost: 0.3889116644859314, Accuracy: 0.8321428298950195\n",
            "Cost: 0.36791568994522095, Accuracy: 0.8392857313156128\n",
            "Cost: 0.3603082299232483, Accuracy: 0.8428571224212646\n",
            "Cost: 0.3833816647529602, Accuracy: 0.8299999833106995\n",
            "Cost: 0.3683406412601471, Accuracy: 0.8396428823471069\n",
            "Cost: 0.36284029483795166, Accuracy: 0.8378571271896362\n",
            "Cost: 0.3606415092945099, Accuracy: 0.8360714316368103\n",
            "Cost: 0.367016464471817, Accuracy: 0.8392857313156128\n",
            "Cost: 0.3580842614173889, Accuracy: 0.8403571248054504\n",
            "Cost: 0.3663296401500702, Accuracy: 0.8371428847312927\n",
            "Cost: 0.36602136492729187, Accuracy: 0.833214282989502\n",
            "Cost: 0.34559741616249084, Accuracy: 0.8510714173316956\n",
            "Cost: 0.35214874148368835, Accuracy: 0.8446428775787354\n",
            "Cost: 0.34296342730522156, Accuracy: 0.852142870426178\n",
            "Cost: 0.3421763777732849, Accuracy: 0.8500000238418579\n",
            "Cost: 0.35312384366989136, Accuracy: 0.8464285731315613\n",
            "Cost: 0.3541635572910309, Accuracy: 0.8478571176528931\n",
            "Cost: 0.34274280071258545, Accuracy: 0.8428571224212646\n",
            "Cost: 0.32637789845466614, Accuracy: 0.8614285588264465\n",
            "Cost: 0.3343777060508728, Accuracy: 0.856071412563324\n",
            "Cost: 0.33799025416374207, Accuracy: 0.8475000262260437\n",
            "Cost: 0.34118905663490295, Accuracy: 0.8553571701049805\n",
            "Cost: 0.33608734607696533, Accuracy: 0.8571428656578064\n",
            "Cost: 0.31615182757377625, Accuracy: 0.864642858505249\n",
            "Cost: 0.3322350084781647, Accuracy: 0.8532142639160156\n",
            "Cost: 0.31854498386383057, Accuracy: 0.8585714101791382\n",
            "Cost: 0.31851720809936523, Accuracy: 0.8564285635948181\n",
            "Cost: 0.32413747906684875, Accuracy: 0.8557142615318298\n",
            "Cost: 0.31228455901145935, Accuracy: 0.8571428656578064\n",
            "Cost: 0.32806161046028137, Accuracy: 0.8532142639160156\n",
            "Cost: 0.32047951221466064, Accuracy: 0.8557142615318298\n",
            "Cost: 0.31971657276153564, Accuracy: 0.8607142567634583\n",
            "Cost: 0.3099650740623474, Accuracy: 0.8610714077949524\n",
            "Cost: 0.3089383840560913, Accuracy: 0.8596428632736206\n",
            "Cost: 0.3229990601539612, Accuracy: 0.8535714149475098\n",
            "Cost: 0.31336989998817444, Accuracy: 0.8650000095367432\n",
            "Cost: 0.303002268075943, Accuracy: 0.8692857027053833\n",
            "Cost: 0.30282771587371826, Accuracy: 0.8635714054107666\n",
            "Cost: 0.29245448112487793, Accuracy: 0.871071457862854\n",
            "Cost: 0.30115869641304016, Accuracy: 0.8650000095367432\n",
            "Cost: 0.2925442159175873, Accuracy: 0.8657143115997314\n",
            "Cost: 0.301636666059494, Accuracy: 0.8671428561210632\n",
            "Cost: 0.2986198961734772, Accuracy: 0.8767856955528259\n",
            "Cost: 0.2927103340625763, Accuracy: 0.8703571557998657\n",
            "Cost: 0.30364856123924255, Accuracy: 0.8803571462631226\n",
            "Cost: 0.3009413778781891, Accuracy: 0.8742856979370117\n",
            "Cost: 0.28891927003860474, Accuracy: 0.8807142972946167\n",
            "Cost: 0.2963702976703644, Accuracy: 0.8810714483261108\n",
            "Cost: 0.2914673388004303, Accuracy: 0.8714285492897034\n",
            "Cost: 0.28926020860671997, Accuracy: 0.8807142972946167\n",
            "Cost: 0.2776211202144623, Accuracy: 0.8828571438789368\n",
            "Cost: 0.2821834981441498, Accuracy: 0.8807142972946167\n",
            "Cost: 0.27382153272628784, Accuracy: 0.8807142972946167\n",
            "Cost: 0.26940110325813293, Accuracy: 0.8828571438789368\n",
            "Cost: 0.2831154763698578, Accuracy: 0.8757143020629883\n",
            "Cost: 0.2720433473587036, Accuracy: 0.8867856860160828\n",
            "Cost: 0.2949336767196655, Accuracy: 0.875\n",
            "Cost: 0.27878254652023315, Accuracy: 0.8878571391105652\n",
            "Cost: 0.26808997988700867, Accuracy: 0.8878571391105652\n",
            "Cost: 0.26194867491722107, Accuracy: 0.8882142901420593\n",
            "Cost: 0.25649937987327576, Accuracy: 0.8849999904632568\n",
            "Cost: 0.2678736746311188, Accuracy: 0.8849999904632568\n",
            "Cost: 0.2710695266723633, Accuracy: 0.8903571367263794\n",
            "Cost: 0.25915709137916565, Accuracy: 0.8914285898208618\n",
            "Cost: 0.25670212507247925, Accuracy: 0.8974999785423279\n",
            "Cost: 0.2624063193798065, Accuracy: 0.8928571343421936\n",
            "Cost: 0.2648618817329407, Accuracy: 0.8932142853736877\n",
            "Cost: 0.25886815786361694, Accuracy: 0.8903571367263794\n",
            "Cost: 0.24816519021987915, Accuracy: 0.8871428370475769\n",
            "Cost: 0.26486894488334656, Accuracy: 0.8889285922050476\n",
            "Cost: 0.25800490379333496, Accuracy: 0.8953571319580078\n",
            "Cost: 0.25845447182655334, Accuracy: 0.8921428322792053\n",
            "Cost: 0.24870041012763977, Accuracy: 0.8899999856948853\n",
            "Cost: 0.24532924592494965, Accuracy: 0.897857129573822\n",
            "Cost: 0.2507741451263428, Accuracy: 0.8924999833106995\n",
            "Cost: 0.24382171034812927, Accuracy: 0.8953571319580078\n",
            "Cost: 0.23924651741981506, Accuracy: 0.9017857313156128\n",
            "Cost: 0.23647864162921906, Accuracy: 0.9035714268684387\n",
            "Cost: 0.23604026436805725, Accuracy: 0.897857129573822\n",
            "Cost: 0.2357172966003418, Accuracy: 0.9014285802841187\n",
            "Cost: 0.24277736246585846, Accuracy: 0.893928587436676\n",
            "Cost: 0.23546770215034485, Accuracy: 0.9014285802841187\n",
            "Cost: 0.23836664855480194, Accuracy: 0.8992857336997986\n",
            "Cost: 0.23190133273601532, Accuracy: 0.9057142734527588\n",
            "Cost: 0.23015476763248444, Accuracy: 0.9092857241630554\n",
            "Cost: 0.2325662225484848, Accuracy: 0.9060714244842529\n",
            "Cost: 0.23127801716327667, Accuracy: 0.9071428775787354\n",
            "Cost: 0.24371328949928284, Accuracy: 0.8921428322792053\n",
            "Cost: 0.2252746969461441, Accuracy: 0.9032142758369446\n",
            "Cost: 0.23129895329475403, Accuracy: 0.9053571224212646\n",
            "Cost: 0.21633349359035492, Accuracy: 0.9139285683631897\n",
            "Cost: 0.22030611336231232, Accuracy: 0.9110714197158813\n",
            "Cost: 0.23196500539779663, Accuracy: 0.9060714244842529\n",
            "Cost: 0.2074216604232788, Accuracy: 0.9114285707473755\n",
            "Cost: 0.22238412499427795, Accuracy: 0.9092857241630554\n",
            "Cost: 0.2198559194803238, Accuracy: 0.9100000262260437\n",
            "Cost: 0.20922772586345673, Accuracy: 0.9128571152687073\n",
            "Cost: 0.20906585454940796, Accuracy: 0.9142857193946838\n",
            "Cost: 0.2133868783712387, Accuracy: 0.9121428728103638\n",
            "Cost: 0.20659257471561432, Accuracy: 0.9171428680419922\n",
            "Cost: 0.21610909700393677, Accuracy: 0.9107142686843872\n",
            "Cost: 0.20087122917175293, Accuracy: 0.9242857098579407\n",
            "Cost: 0.20441628992557526, Accuracy: 0.9153571724891663\n",
            "Cost: 0.20564547181129456, Accuracy: 0.9182142615318298\n",
            "Cost: 0.2081088274717331, Accuracy: 0.9132142663002014\n",
            "Cost: 0.22620344161987305, Accuracy: 0.9103571176528931\n",
            "Cost: 0.19012568891048431, Accuracy: 0.9228571653366089\n",
            "Cost: 0.19100406765937805, Accuracy: 0.925000011920929\n",
            "Cost: 0.2071593999862671, Accuracy: 0.918571412563324\n",
            "Cost: 0.19834651052951813, Accuracy: 0.9182142615318298\n",
            "Cost: 0.20472948253154755, Accuracy: 0.9160714149475098\n",
            "Cost: 0.19539213180541992, Accuracy: 0.9200000166893005\n",
            "Cost: 0.20073752105236053, Accuracy: 0.9139285683631897\n",
            "Cost: 0.19923749566078186, Accuracy: 0.9182142615318298\n",
            "Cost: 0.1960630565881729, Accuracy: 0.9253571629524231\n",
            "Cost: 0.2038014829158783, Accuracy: 0.9164285659790039\n",
            "Cost: 0.1954912394285202, Accuracy: 0.9214285612106323\n",
            "Cost: 0.18335530161857605, Accuracy: 0.927142858505249\n",
            "Cost: 0.1938103884458542, Accuracy: 0.920714259147644\n",
            "Cost: 0.19741906225681305, Accuracy: 0.9175000190734863\n",
            "Cost: 0.1840287297964096, Accuracy: 0.9303571581840515\n",
            "Cost: 0.1885560005903244, Accuracy: 0.9235714077949524\n",
            "Cost: 0.17970290780067444, Accuracy: 0.9260714054107666\n",
            "Cost: 0.18917407095432281, Accuracy: 0.9278571605682373\n",
            "Cost: 0.186299130320549, Accuracy: 0.9221428632736206\n",
            "Cost: 0.18487417697906494, Accuracy: 0.9278571605682373\n",
            "Cost: 0.17521633207798004, Accuracy: 0.9296428561210632\n",
            "Cost: 0.17924001812934875, Accuracy: 0.928928554058075\n",
            "Cost: 0.16263477504253387, Accuracy: 0.9342857003211975\n",
            "Cost: 0.1742948740720749, Accuracy: 0.9325000047683716\n",
            "Cost: 0.17399121820926666, Accuracy: 0.9278571605682373\n",
            "Cost: 0.18367396295070648, Accuracy: 0.931071400642395\n",
            "Cost: 0.15624050796031952, Accuracy: 0.9364285469055176\n",
            "Cost: 0.17084692418575287, Accuracy: 0.9407142996788025\n",
            "Cost: 0.17881588637828827, Accuracy: 0.9328571557998657\n",
            "Cost: 0.16704261302947998, Accuracy: 0.9328571557998657\n",
            "Cost: 0.1738724410533905, Accuracy: 0.9267857074737549\n",
            "Cost: 0.16637572646141052, Accuracy: 0.9325000047683716\n",
            "Cost: 0.17086000740528107, Accuracy: 0.9285714030265808\n",
            "Cost: 0.1596030294895172, Accuracy: 0.9314285516738892\n",
            "Cost: 0.15968473255634308, Accuracy: 0.9389285445213318\n",
            "Cost: 0.16589100658893585, Accuracy: 0.9385714530944824\n",
            "Cost: 0.16821499168872833, Accuracy: 0.9350000023841858\n",
            "Cost: 0.16728726029396057, Accuracy: 0.9371428489685059\n",
            "Cost: 0.1542438119649887, Accuracy: 0.9421428442001343\n",
            "Cost: 0.1516522765159607, Accuracy: 0.941428542137146\n",
            "Cost: 0.16136527061462402, Accuracy: 0.9321428537368774\n",
            "Cost: 0.15659257769584656, Accuracy: 0.9357143044471741\n",
            "Cost: 0.1625043749809265, Accuracy: 0.9317857027053833\n",
            "Cost: 0.15380840003490448, Accuracy: 0.9442856907844543\n",
            "Cost: 0.15364791452884674, Accuracy: 0.9367856979370117\n",
            "Cost: 0.1488182544708252, Accuracy: 0.9464285969734192\n",
            "Cost: 0.147879496216774, Accuracy: 0.9453571438789368\n",
            "Cost: 0.164886012673378, Accuracy: 0.9342857003211975\n",
            "Cost: 0.1561904400587082, Accuracy: 0.943928599357605\n",
            "Cost: 0.14285872876644135, Accuracy: 0.946071445941925\n",
            "Cost: 0.15007618069648743, Accuracy: 0.9367856979370117\n",
            "Cost: 0.16380557417869568, Accuracy: 0.9350000023841858\n",
            "Cost: 0.1537351757287979, Accuracy: 0.9396428465843201\n",
            "Cost: 0.14087644219398499, Accuracy: 0.946071445941925\n",
            "Cost: 0.1394902765750885, Accuracy: 0.9457142949104309\n",
            "Cost: 0.14472980797290802, Accuracy: 0.941428542137146\n",
            "Cost: 0.15319375693798065, Accuracy: 0.9389285445213318\n",
            "Cost: 0.15667560696601868, Accuracy: 0.9396428465843201\n",
            "Cost: 0.13590385019779205, Accuracy: 0.9532142877578735\n",
            "Cost: 0.1362084150314331, Accuracy: 0.9507142901420593\n",
            "Cost: 0.1398576945066452, Accuracy: 0.9507142901420593\n",
            "Cost: 0.14789226651191711, Accuracy: 0.9435714483261108\n",
            "Cost: 0.12993884086608887, Accuracy: 0.9549999833106995\n",
            "Cost: 0.14326463639736176, Accuracy: 0.9446428418159485\n",
            "Cost: 0.13622477650642395, Accuracy: 0.9482142925262451\n",
            "Cost: 0.14125515520572662, Accuracy: 0.9442856907844543\n",
            "Cost: 0.13947409391403198, Accuracy: 0.9453571438789368\n",
            "Cost: 0.13212819397449493, Accuracy: 0.9535714387893677\n",
            "Cost: 0.12608736753463745, Accuracy: 0.9546428322792053\n",
            "Cost: 0.12597119808197021, Accuracy: 0.9567857384681702\n",
            "Cost: 0.1186356469988823, Accuracy: 0.9589285850524902\n",
            "Cost: 0.12233579158782959, Accuracy: 0.9514285922050476\n",
            "Cost: 0.13876841962337494, Accuracy: 0.9482142925262451\n",
            "Cost: 0.12906453013420105, Accuracy: 0.9503571391105652\n",
            "Cost: 0.11606037616729736, Accuracy: 0.9596428275108337\n",
            "Cost: 0.1278185397386551, Accuracy: 0.9474999904632568\n",
            "Cost: 0.11834067106246948, Accuracy: 0.960357129573822\n",
            "Cost: 0.1300818920135498, Accuracy: 0.9521428346633911\n",
            "Cost: 0.1312684416770935, Accuracy: 0.9528571367263794\n",
            "Cost: 0.12210492789745331, Accuracy: 0.9517857432365417\n",
            "Cost: 0.1286858767271042, Accuracy: 0.9503571391105652\n",
            "Cost: 0.11741697043180466, Accuracy: 0.9571428298950195\n",
            "Cost: 0.11964979022741318, Accuracy: 0.9567857384681702\n",
            "Cost: 0.11814173310995102, Accuracy: 0.9592857360839844\n",
            "Cost: 0.12372429668903351, Accuracy: 0.9560714364051819\n",
            "Cost: 0.12596306204795837, Accuracy: 0.9528571367263794\n",
            "Cost: 0.11238296329975128, Accuracy: 0.9571428298950195\n",
            "Cost: 0.1326017677783966, Accuracy: 0.9524999856948853\n",
            "Cost: 0.11659637093544006, Accuracy: 0.9585714340209961\n",
            "Cost: 0.12161479145288467, Accuracy: 0.9489285945892334\n",
            "Cost: 0.10993660241365433, Accuracy: 0.9599999785423279\n",
            "Cost: 0.12583307921886444, Accuracy: 0.9532142877578735\n",
            "Cost: 0.1107693687081337, Accuracy: 0.9621428847312927\n",
            "Cost: 0.10966694355010986, Accuracy: 0.9628571271896362\n",
            "Cost: 0.11019834131002426, Accuracy: 0.9628571271896362\n",
            "Cost: 0.11329259723424911, Accuracy: 0.9553571343421936\n",
            "Cost: 0.11381953954696655, Accuracy: 0.9592857360839844\n",
            "Cost: 0.11789456009864807, Accuracy: 0.9557142853736877\n",
            "Cost: 0.110427126288414, Accuracy: 0.9617857336997986\n",
            "Cost: 0.10183703899383545, Accuracy: 0.9649999737739563\n",
            "Cost: 0.10139310359954834, Accuracy: 0.9674999713897705\n",
            "Cost: 0.11162105202674866, Accuracy: 0.9585714340209961\n",
            "Cost: 0.11662030965089798, Accuracy: 0.9596428275108337\n",
            "Cost: 0.10108976811170578, Accuracy: 0.9632142782211304\n",
            "Cost: 0.10827429592609406, Accuracy: 0.9599999785423279\n",
            "Cost: 0.10823089629411697, Accuracy: 0.9557142853736877\n",
            "Cost: 0.09728513658046722, Accuracy: 0.9671428799629211\n",
            "Cost: 0.11441867053508759, Accuracy: 0.9610714316368103\n",
            "Cost: 0.1081630289554596, Accuracy: 0.9617857336997986\n",
            "Cost: 0.10758604109287262, Accuracy: 0.960357129573822\n",
            "Cost: 0.09681820124387741, Accuracy: 0.9678571224212646\n",
            "Cost: 0.10899335891008377, Accuracy: 0.9599999785423279\n",
            "Cost: 0.09519375115633011, Accuracy: 0.9657142758369446\n",
            "Cost: 0.10088082402944565, Accuracy: 0.9621428847312927\n",
            "Cost: 0.09820890426635742, Accuracy: 0.9639285802841187\n",
            "Cost: 0.09264816343784332, Accuracy: 0.9696428775787354\n",
            "Cost: 0.09860467165708542, Accuracy: 0.9660714268684387\n",
            "Cost: 0.1050405204296112, Accuracy: 0.9614285826683044\n",
            "Cost: 0.08381592482328415, Accuracy: 0.9739285707473755\n",
            "Cost: 0.0947398915886879, Accuracy: 0.9653571248054504\n",
            "Cost: 0.09632154554128647, Accuracy: 0.9639285802841187\n",
            "Cost: 0.09687383472919464, Accuracy: 0.9660714268684387\n",
            "Cost: 0.0922245979309082, Accuracy: 0.9678571224212646\n",
            "Cost: 0.10204887390136719, Accuracy: 0.9614285826683044\n",
            "Cost: 0.10258028656244278, Accuracy: 0.9653571248054504\n",
            "Cost: 0.1014927551150322, Accuracy: 0.9635714292526245\n",
            "Cost: 0.08616235852241516, Accuracy: 0.9696428775787354\n",
            "Cost: 0.10363519936800003, Accuracy: 0.9596428275108337\n",
            "Cost: 0.0934058278799057, Accuracy: 0.9678571224212646\n",
            "Cost: 0.0876789391040802, Accuracy: 0.9700000286102295\n",
            "Cost: 0.09470091760158539, Accuracy: 0.9646428823471069\n",
            "Cost: 0.09014905244112015, Accuracy: 0.9657142758369446\n",
            "Cost: 0.08419396728277206, Accuracy: 0.9725000262260437\n",
            "Cost: 0.09668006002902985, Accuracy: 0.9682142734527588\n",
            "Cost: 0.08168168365955353, Accuracy: 0.9764285683631897\n",
            "Cost: 0.09517297893762589, Accuracy: 0.9671428799629211\n",
            "Cost: 0.0803229883313179, Accuracy: 0.9703571200370789\n",
            "Cost: 0.0810585469007492, Accuracy: 0.9717857241630554\n",
            "Cost: 0.08016647398471832, Accuracy: 0.9725000262260437\n",
            "Cost: 0.08991224318742752, Accuracy: 0.9682142734527588\n",
            "Cost: 0.08793342858552933, Accuracy: 0.9710714221000671\n",
            "Cost: 0.07243257761001587, Accuracy: 0.9757142663002014\n",
            "Cost: 0.08457867056131363, Accuracy: 0.9714285731315613\n",
            "Cost: 0.08678904175758362, Accuracy: 0.9689285755157471\n",
            "Cost: 0.08372422307729721, Accuracy: 0.9717857241630554\n",
            "Cost: 0.09173905849456787, Accuracy: 0.9671428799629211\n",
            "Cost: 0.07152877748012543, Accuracy: 0.977142870426178\n",
            "Cost: 0.07894863933324814, Accuracy: 0.9717857241630554\n",
            "Cost: 0.08933432400226593, Accuracy: 0.9671428799629211\n",
            "Cost: 0.08875459432601929, Accuracy: 0.9682142734527588\n",
            "Cost: 0.08351782709360123, Accuracy: 0.9692857265472412\n",
            "Cost: 0.08123642206192017, Accuracy: 0.9725000262260437\n",
            "Cost: 0.07908672839403152, Accuracy: 0.9753571152687073\n",
            "Cost: 0.07897911965847015, Accuracy: 0.9721428751945496\n",
            "Cost: 0.07528998702764511, Accuracy: 0.9800000190734863\n",
            "Cost: 0.07760294526815414, Accuracy: 0.9735714197158813\n",
            "Cost: 0.0771082267165184, Accuracy: 0.970714271068573\n",
            "Cost: 0.08400402963161469, Accuracy: 0.970714271068573\n",
            "Cost: 0.07550621777772903, Accuracy: 0.9742857217788696\n",
            "Cost: 0.07723482698202133, Accuracy: 0.9739285707473755\n",
            "Cost: 0.07759968936443329, Accuracy: 0.9728571176528931\n",
            "Cost: 0.07645969837903976, Accuracy: 0.9714285731315613\n",
            "Cost: 0.07703932374715805, Accuracy: 0.9775000214576721\n",
            "Cost: 0.07571340352296829, Accuracy: 0.9714285731315613\n",
            "Cost: 0.0697326734662056, Accuracy: 0.9778571724891663\n",
            "Cost: 0.07050961256027222, Accuracy: 0.9767857193946838\n",
            "Cost: 0.0810815840959549, Accuracy: 0.9735714197158813\n",
            "Cost: 0.06722475588321686, Accuracy: 0.9796428680419922\n",
            "Cost: 0.0744241252541542, Accuracy: 0.9739285707473755\n",
            "Cost: 0.06849031895399094, Accuracy: 0.9817857146263123\n",
            "Cost: 0.08249778300523758, Accuracy: 0.9696428775787354\n",
            "Cost: 0.07716108858585358, Accuracy: 0.9746428728103638\n",
            "Cost: 0.06330668181180954, Accuracy: 0.9796428680419922\n",
            "Cost: 0.07297448068857193, Accuracy: 0.9796428680419922\n",
            "Cost: 0.08047014474868774, Accuracy: 0.970714271068573\n",
            "Cost: 0.07085665315389633, Accuracy: 0.977142870426178\n",
            "Cost: 0.06783849745988846, Accuracy: 0.977142870426178\n",
            "Cost: 0.07554389536380768, Accuracy: 0.9764285683631897\n",
            "Cost: 0.06648121774196625, Accuracy: 0.9785714149475098\n",
            "Cost: 0.06691033393144608, Accuracy: 0.9778571724891663\n",
            "Cost: 0.06928230077028275, Accuracy: 0.9778571724891663\n",
            "Cost: 0.07148554921150208, Accuracy: 0.9746428728103638\n",
            "Cost: 0.07054439932107925, Accuracy: 0.9757142663002014\n",
            "Cost: 0.0605204813182354, Accuracy: 0.9789285659790039\n",
            "Cost: 0.06755949556827545, Accuracy: 0.977142870426178\n",
            "Cost: 0.0694412961602211, Accuracy: 0.9796428680419922\n",
            "Cost: 0.06922801584005356, Accuracy: 0.977142870426178\n",
            "Cost: 0.07105773687362671, Accuracy: 0.9746428728103638\n",
            "Cost: 0.06512602418661118, Accuracy: 0.9782142639160156\n",
            "Cost: 0.06569238007068634, Accuracy: 0.9782142639160156\n",
            "Cost: 0.06468730419874191, Accuracy: 0.9782142639160156\n",
            "Cost: 0.06529658287763596, Accuracy: 0.979285717010498\n",
            "Cost: 0.0605383962392807, Accuracy: 0.979285717010498\n",
            "Cost: 0.06971999257802963, Accuracy: 0.9753571152687073\n",
            "Cost: 0.06562329083681107, Accuracy: 0.9735714197158813\n",
            "Cost: 0.06699684262275696, Accuracy: 0.9803571701049805\n",
            "Cost: 0.054423969238996506, Accuracy: 0.9821428656578064\n",
            "Cost: 0.06763263046741486, Accuracy: 0.9764285683631897\n",
            "Cost: 0.05500733479857445, Accuracy: 0.9825000166893005\n",
            "Cost: 0.06045813858509064, Accuracy: 0.9803571701049805\n",
            "Cost: 0.060906678438186646, Accuracy: 0.981071412563324\n",
            "Cost: 0.06280072778463364, Accuracy: 0.9807142615318298\n",
            "Cost: 0.06108936294913292, Accuracy: 0.979285717010498\n",
            "Cost: 0.06225069984793663, Accuracy: 0.981071412563324\n",
            "Cost: 0.04859839379787445, Accuracy: 0.9853571653366089\n",
            "Cost: 0.06247028335928917, Accuracy: 0.9817857146263123\n",
            "Cost: 0.05302117019891739, Accuracy: 0.9828571677207947\n",
            "Cost: 0.05848686397075653, Accuracy: 0.9814285635948181\n",
            "Cost: 0.06308554857969284, Accuracy: 0.9778571724891663\n",
            "Cost: 0.057487182319164276, Accuracy: 0.9821428656578064\n",
            "Cost: 0.052096687257289886, Accuracy: 0.9821428656578064\n",
            "Cost: 0.056672099977731705, Accuracy: 0.9817857146263123\n",
            "Cost: 0.0566064789891243, Accuracy: 0.9850000143051147\n",
            "Cost: 0.05179232358932495, Accuracy: 0.9828571677207947\n",
            "Cost: 0.05104754865169525, Accuracy: 0.9835714101791382\n",
            "Cost: 0.05655227601528168, Accuracy: 0.9821428656578064\n",
            "Cost: 0.05242147296667099, Accuracy: 0.9842857122421265\n",
            "Cost: 0.05338519439101219, Accuracy: 0.9814285635948181\n",
            "Cost: 0.06246970221400261, Accuracy: 0.9789285659790039\n",
            "Cost: 0.04890379682183266, Accuracy: 0.9835714101791382\n",
            "Cost: 0.054253898561000824, Accuracy: 0.9814285635948181\n",
            "Cost: 0.05560314655303955, Accuracy: 0.9842857122421265\n",
            "Cost: 0.05203625187277794, Accuracy: 0.9825000166893005\n",
            "Cost: 0.061160583049058914, Accuracy: 0.9800000190734863\n",
            "Cost: 0.04963669925928116, Accuracy: 0.9846428632736206\n",
            "Cost: 0.05640454962849617, Accuracy: 0.9807142615318298\n",
            "Cost: 0.05336083099246025, Accuracy: 0.9839285612106323\n",
            "Cost: 0.05921672284603119, Accuracy: 0.9789285659790039\n",
            "Cost: 0.050989292562007904, Accuracy: 0.9846428632736206\n",
            "Cost: 0.05095844715833664, Accuracy: 0.9846428632736206\n",
            "Cost: 0.05907784774899483, Accuracy: 0.9814285635948181\n",
            "Cost: 0.05112915486097336, Accuracy: 0.9850000143051147\n",
            "Cost: 0.055082909762859344, Accuracy: 0.9817857146263123\n",
            "Cost: 0.05736405402421951, Accuracy: 0.9814285635948181\n",
            "Cost: 0.051708340644836426, Accuracy: 0.9825000166893005\n",
            "Cost: 0.047099217772483826, Accuracy: 0.987500011920929\n",
            "Cost: 0.053452931344509125, Accuracy: 0.9821428656578064\n",
            "0.825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}