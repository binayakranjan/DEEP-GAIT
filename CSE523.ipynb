{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE523.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binayakranjan/DEEP-GAIT/blob/master/CSE523.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex3qQJGbaAxb",
        "colab_type": "code",
        "outputId": "56a88b6f-3908-42b1-be11-7547e688ec28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount your google drive where you've saved your assignment folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aupcPLfnaCw2",
        "colab_type": "code",
        "outputId": "4b0ed652-d276-4375-faf1-73c8ceccb00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/gdrive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtlSJWosx06m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "outputId": "46d73b8c-71ac-46cc-8188-a37a356a54d4"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1014-practice-questions.pdf\n",
            " 191021-serializability.mov\n",
            "'5 lb. Book of GRE Practice Problems - Manhattan Prep.pdf'\n",
            " AIMCAT12.zip\n",
            " Barrons_New_GRE_19th_ed.pdf\n",
            " Binayak_Additional_info.docx\n",
            "'BinayakDasCv - binayakranjan das.pdf'\n",
            " BinayakDasCv.pdf\n",
            " BinayakRanjanDas.pdf\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "\u001b[01;34m'Google Buzz'\u001b[0m/\n",
            "'GRE Math Bible.pdf NOVA.pdf'\n",
            "'Lic Receipt.jpg'\n",
            " MAGOOSH+589+QUANT+PRACTICE+QUESTIONS.pdf\n",
            " main.pdf\n",
            "'Monthly Expenditure.gsheet'\n",
            "'mu sigma id.gsheet'\n",
            "\u001b[01;34m'New Folder'\u001b[0m/\n",
            "'passport .pdf'\n",
            "'Photo album.gslides'\n",
            "'photo immigration .pdf'\n",
            "'Resume (1).gdoc'\n",
            "'Resume (2).gdoc'\n",
            "'Resume (3).gdoc'\n",
            "'Resume (4).gdoc'\n",
            "'Resume (5).gdoc'\n",
            "'Resume (6).gdoc'\n",
            "'Resume (7).gdoc'\n",
            " Resume.gdoc\n",
            " run.pkl\n",
            "'sapient test results.gsheet'\n",
            " Scanned_20190820-1133.pdf\n",
            " \u001b[01;34mSHIVSAMUDRAM\u001b[0m/\n",
            " stonyVisa.pdf\n",
            " store_data.csv\n",
            " \u001b[01;34mSW-DESIGN\u001b[0m/\n",
            " \u001b[01;34mtest\u001b[0m/\n",
            " \u001b[01;34mtest2\u001b[0m/\n",
            " Test_data.csv\n",
            " \u001b[01;34mtrain\u001b[0m/\n",
            " Training_data.csv\n",
            "'Untitled document.gdoc'\n",
            "'Untitled form [Form].gform'\n",
            "'Untitled form.gsheet'\n",
            "'Untitled spreadsheet.gsheet'\n",
            " walk.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31Ovu4h1vWrm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "f351f414-5ec8-4248-8d70-c2f1579abbe5"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCwE8E-Bv7t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data_path, batch_size):\n",
        "\n",
        "    data = np.load(data_path, allow_pickle = True)\n",
        "    num_class = len(data)\n",
        "\n",
        "    pairs = []\n",
        "    pair_idx = []\n",
        "    labels = []\n",
        "\n",
        "    for user_idx, user_data in enumerate(data):\n",
        "\n",
        "        user_label = np.zeros(batch_size)\n",
        "        user_label[:len(user_label)//2] = 1\n",
        "\n",
        "        num_data = len(user_data)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "\n",
        "            batch = []\n",
        "            batch_idx = [user_idx]\n",
        "            random_idx = random.randint(0, num_data-1)\n",
        "            batch.append(user_data[random_idx])\n",
        "\n",
        "            if idx < len(user_label)//2:\n",
        "                random_idx = random.randint(0, num_data-1)\n",
        "                batch.append(user_data[random_idx])\n",
        "                batch_idx.append(user_idx)\n",
        "            else:\n",
        "                random_user = (user_idx + random.randint(1,num_class-1)) % num_class\n",
        "                random_user_data = data[random_user]\n",
        "\n",
        "                random_idx = random.randint(0, len(random_user_data)-1)\n",
        "                batch.append(random_user_data[random_idx])\n",
        "                batch_idx.append(random_user)\n",
        "\n",
        "            pairs.append(batch)\n",
        "            pair_idx.append(batch_idx)\n",
        "        labels.append(user_label)\n",
        "\n",
        "\n",
        "    return np.array(pairs), np.array(pair_idx), np.array(labels).ravel().reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ow5e_l9wCD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_data(data, index, label, ratio):\n",
        "\n",
        "    data, index, label = shuffle(data, index, label)\n",
        "\n",
        "    train_data, train_index, train_label = data[:int(len(data)*ratio)], index[:int(len(data)*ratio)], label[:int(len(data)*ratio)]\n",
        "    test_data, test_index, test_label = data[int(len(data)*ratio):], index[int(len(data)*ratio):], label[int(len(data)*ratio):]\n",
        "\n",
        "    return train_data, train_index, train_label, test_data, test_index, test_label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A4JTPucwE9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(data, index, label, ratio):\n",
        "\n",
        "    train_data, train_index, train_label, test_data, test_index, test_label = shuffle_data(data, index, label, ratio)\n",
        "\n",
        "    while np.any(np.unique(train_index) != np.unique(index)):\n",
        "\n",
        "        train_data, train_index, train_label, test_data, test_index, test_label = shuffle_data(data, index, label, ratio)\n",
        "\n",
        "    return train_data, train_index, train_label, test_data, test_index, test_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRRU715FwLKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "The output of Siamese network would be similarity score, which indicates a pair of gait cycle are from the same user or not (binary classification)\n",
        "'''\n",
        "def Layer(X, num_output, initializer, keep_prob, W_name, b_name):\n",
        "\n",
        "    _, num_feature = X.shape\n",
        "\n",
        "    W = tf.get_variable(W_name, shape = [num_feature, num_output], dtype = tf.float32, initializer = initializer)\n",
        "    b = tf.Variable(tf.random_normal([num_output]), name = b_name)\n",
        "    L = tf.matmul(X, W) + b\n",
        "    L = tf.nn.relu(L)\n",
        "    L = tf.nn.dropout(L, keep_prob = keep_prob)\n",
        "\n",
        "    return L\n",
        "\n",
        "def siamese(input_data, keep_prob, reuse = False):\n",
        "\n",
        "    l1_dim = 2000\n",
        "    l2_dim = 3000\n",
        "    l3_dim = 3000\n",
        "\n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "    with tf.variable_scope('Layer1', reuse = reuse) as scope:\n",
        "        model = Layer(input_data, l1_dim, initializer, keep_prob, 'W1', 'b1')\n",
        "\n",
        "    with tf.variable_scope('Layer2', reuse = reuse) as scope:\n",
        "        model = Layer(model, l2_dim, initializer, keep_prob, 'W2', 'b2')\n",
        "\n",
        "    with tf.variable_scope('Layer3', reuse = reuse) as scope:\n",
        "        model = Layer(model, l3_dim, initializer, keep_prob, 'W3', 'b3')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwS9KhUUvL9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355c4f65-94ea-44b1-af1a-413d6a0dc6e8"
      },
      "source": [
        "#walk_data_path = 'data/walk/filtered_interpolation.pkl'\n",
        "walk_data_path = 'run.pkl'\n",
        "batch_size = 80\n",
        "num_iter = 500\n",
        "initial_learning_rate = 10**(-4)\n",
        "\n",
        "pairs, pair_idx, labels = get_data(walk_data_path, batch_size)\n",
        "num_pairs, pair_size, cycle_length, num_feature = pairs.shape\n",
        "print(labels.shape)\n",
        "train_data, train_index, train_label, test_data, test_index, test_label = train_test_split(pairs, pair_idx, labels, 0.7)\n",
        "print(train_data.shape, train_index.shape, train_label.shape)\n",
        "\n",
        "left = tf.placeholder(tf.float32, shape = [None, cycle_length, num_feature], name = 'left')\n",
        "right = tf.placeholder(tf.float32, shape = [None, cycle_length, num_feature], name = 'right')\n",
        "\n",
        "new_left = tf.reshape(left, [-1, cycle_length*num_feature])\n",
        "new_right = tf.reshape(right, [-1, cycle_length*num_feature])\n",
        "\n",
        "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "\n",
        "left_model = siamese(new_left, keep_prob, False)\n",
        "right_model = siamese(new_right, keep_prob, True)\n",
        "\n",
        "with tf.variable_scope('Difference'):\n",
        "    difference = tf.math.abs(left_model - right_model)\n",
        "\n",
        "with tf.variable_scope('Dense'):\n",
        "\n",
        "    W = tf.get_variable('W', shape = [difference.shape[-1], 1], dtype = tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
        "    b = tf.Variable(tf.random_normal([1]), name = 'b')\n",
        "    L = tf.matmul(difference, W) + b\n",
        "\n",
        "with tf.name_scope('Training'):\n",
        "\n",
        "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = L, labels = Y))\n",
        "\n",
        "    global_step = tf.Variable(0)\n",
        "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, 100, 0.9)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
        "\n",
        "    similarity_score = tf.nn.sigmoid(L)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(similarity_score), Y), tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for iter in range(num_iter):\n",
        "\n",
        "        c, a, _ = sess.run([cost, accuracy, optimizer], feed_dict = {left: train_data[:, 0, :, :], right: train_data[:, 1, :, :], Y: train_label, keep_prob: 0.7})\n",
        "        print('Cost: {}, Accuracy: {}'.format(c, a))\n",
        "\n",
        "    acc = sess.run(accuracy, feed_dict = {left: test_data[:, 0, :, :], right: test_data[:, 1, :, :], Y: test_label, keep_prob: 1.0})\n",
        "    print(acc)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4000, 1)\n",
            "(2800, 2, 200, 6) (2800, 2) (2800, 1)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-a3448ab9cda2>:13: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Cost: 2.6724627017974854, Accuracy: 0.5010714530944824\n",
            "Cost: 16.291135787963867, Accuracy: 0.5185714364051819\n",
            "Cost: 10.936062812805176, Accuracy: 0.5185714364051819\n",
            "Cost: 2.143188238143921, Accuracy: 0.5335714221000671\n",
            "Cost: 7.671644687652588, Accuracy: 0.4814285635948181\n",
            "Cost: 9.888044357299805, Accuracy: 0.4814285635948181\n",
            "Cost: 8.172115325927734, Accuracy: 0.4814285635948181\n",
            "Cost: 4.538261890411377, Accuracy: 0.48178571462631226\n",
            "Cost: 1.4407631158828735, Accuracy: 0.5621428489685059\n",
            "Cost: 3.414294719696045, Accuracy: 0.5274999737739563\n",
            "Cost: 4.910003185272217, Accuracy: 0.518928587436676\n",
            "Cost: 4.782761096954346, Accuracy: 0.5185714364051819\n",
            "Cost: 3.567656993865967, Accuracy: 0.5253571271896362\n",
            "Cost: 1.7552882432937622, Accuracy: 0.5628571510314941\n",
            "Cost: 1.2089365720748901, Accuracy: 0.5778571367263794\n",
            "Cost: 2.1481261253356934, Accuracy: 0.5146428346633911\n",
            "Cost: 2.798370122909546, Accuracy: 0.49785715341567993\n",
            "Cost: 2.755375862121582, Accuracy: 0.49464285373687744\n",
            "Cost: 2.1371428966522217, Accuracy: 0.5132142901420593\n",
            "Cost: 1.2881194353103638, Accuracy: 0.5639285445213318\n",
            "Cost: 0.9514015316963196, Accuracy: 0.6235714554786682\n",
            "Cost: 1.324447512626648, Accuracy: 0.5914285778999329\n",
            "Cost: 1.719199538230896, Accuracy: 0.5610714554786682\n",
            "Cost: 1.794210433959961, Accuracy: 0.5667856931686401\n",
            "Cost: 1.4837992191314697, Accuracy: 0.5828571319580078\n",
            "Cost: 1.0785273313522339, Accuracy: 0.604285717010498\n",
            "Cost: 0.8294730186462402, Accuracy: 0.6299999952316284\n",
            "Cost: 0.9236642718315125, Accuracy: 0.6064285635948181\n",
            "Cost: 1.1758956909179688, Accuracy: 0.5821428298950195\n",
            "Cost: 1.2465996742248535, Accuracy: 0.5639285445213318\n",
            "Cost: 1.1532642841339111, Accuracy: 0.5703571438789368\n",
            "Cost: 0.9350757002830505, Accuracy: 0.6003571152687073\n",
            "Cost: 0.8059718608856201, Accuracy: 0.6296428442001343\n",
            "Cost: 0.7744626402854919, Accuracy: 0.6617857217788696\n",
            "Cost: 0.8673456907272339, Accuracy: 0.6453571319580078\n",
            "Cost: 0.9526962041854858, Accuracy: 0.6282142996788025\n",
            "Cost: 0.9594711065292358, Accuracy: 0.6349999904632568\n",
            "Cost: 0.8399253487586975, Accuracy: 0.6589285731315613\n",
            "Cost: 0.7566190958023071, Accuracy: 0.6660714149475098\n",
            "Cost: 0.7274006605148315, Accuracy: 0.666785717010498\n",
            "Cost: 0.7160589694976807, Accuracy: 0.6725000143051147\n",
            "Cost: 0.7822251915931702, Accuracy: 0.6421428322792053\n",
            "Cost: 0.7949007749557495, Accuracy: 0.6453571319580078\n",
            "Cost: 0.7853004932403564, Accuracy: 0.6482142806053162\n",
            "Cost: 0.6970293521881104, Accuracy: 0.6760714054107666\n",
            "Cost: 0.6910513043403625, Accuracy: 0.6825000047683716\n",
            "Cost: 0.6759271025657654, Accuracy: 0.6892856955528259\n",
            "Cost: 0.6935756206512451, Accuracy: 0.6907142996788025\n",
            "Cost: 0.7403293251991272, Accuracy: 0.6678571701049805\n",
            "Cost: 0.7088634371757507, Accuracy: 0.6803571581840515\n",
            "Cost: 0.6779542565345764, Accuracy: 0.6825000047683716\n",
            "Cost: 0.64351487159729, Accuracy: 0.7053571343421936\n",
            "Cost: 0.6727306842803955, Accuracy: 0.6867856979370117\n",
            "Cost: 0.6566680073738098, Accuracy: 0.6778571605682373\n",
            "Cost: 0.6491299867630005, Accuracy: 0.6892856955528259\n",
            "Cost: 0.6529324054718018, Accuracy: 0.6892856955528259\n",
            "Cost: 0.6222348213195801, Accuracy: 0.7142857313156128\n",
            "Cost: 0.6286696791648865, Accuracy: 0.6996428370475769\n",
            "Cost: 0.6106314063072205, Accuracy: 0.7046428322792053\n",
            "Cost: 0.6315562725067139, Accuracy: 0.7185714244842529\n",
            "Cost: 0.6088179349899292, Accuracy: 0.7225000262260437\n",
            "Cost: 0.6236464381217957, Accuracy: 0.7074999809265137\n",
            "Cost: 0.623798668384552, Accuracy: 0.7049999833106995\n",
            "Cost: 0.613776683807373, Accuracy: 0.729285717010498\n",
            "Cost: 0.6202335357666016, Accuracy: 0.7182142734527588\n",
            "Cost: 0.6019282937049866, Accuracy: 0.7057142853736877\n",
            "Cost: 0.5981642603874207, Accuracy: 0.7192857265472412\n",
            "Cost: 0.6000938415527344, Accuracy: 0.7146428823471069\n",
            "Cost: 0.5827426314353943, Accuracy: 0.7364285588264465\n",
            "Cost: 0.5779949426651001, Accuracy: 0.7214285731315613\n",
            "Cost: 0.58195960521698, Accuracy: 0.7389285564422607\n",
            "Cost: 0.5801689028739929, Accuracy: 0.7289285659790039\n",
            "Cost: 0.5542154312133789, Accuracy: 0.7335714101791382\n",
            "Cost: 0.5731353163719177, Accuracy: 0.7364285588264465\n",
            "Cost: 0.5749309062957764, Accuracy: 0.741428554058075\n",
            "Cost: 0.5602856278419495, Accuracy: 0.7492856979370117\n",
            "Cost: 0.5578786730766296, Accuracy: 0.737500011920929\n",
            "Cost: 0.5448353886604309, Accuracy: 0.7478571534156799\n",
            "Cost: 0.5554928183555603, Accuracy: 0.7432143092155457\n",
            "Cost: 0.5269036293029785, Accuracy: 0.75\n",
            "Cost: 0.5093224048614502, Accuracy: 0.7528571486473083\n",
            "Cost: 0.5615251064300537, Accuracy: 0.7360714077949524\n",
            "Cost: 0.5239916443824768, Accuracy: 0.7510714530944824\n",
            "Cost: 0.5386629104614258, Accuracy: 0.7467857003211975\n",
            "Cost: 0.515764594078064, Accuracy: 0.7614285945892334\n",
            "Cost: 0.5302496552467346, Accuracy: 0.7592856884002686\n",
            "Cost: 0.542611300945282, Accuracy: 0.7489285469055176\n",
            "Cost: 0.5229287147521973, Accuracy: 0.7567856907844543\n",
            "Cost: 0.488307923078537, Accuracy: 0.7682142853736877\n",
            "Cost: 0.5017543435096741, Accuracy: 0.7735714316368103\n",
            "Cost: 0.5278145670890808, Accuracy: 0.7635714411735535\n",
            "Cost: 0.49976614117622375, Accuracy: 0.7671428322792053\n",
            "Cost: 0.5200759172439575, Accuracy: 0.7653571367263794\n",
            "Cost: 0.5091564655303955, Accuracy: 0.7660714387893677\n",
            "Cost: 0.49930253624916077, Accuracy: 0.7632142901420593\n",
            "Cost: 0.47590696811676025, Accuracy: 0.7807142734527588\n",
            "Cost: 0.48352518677711487, Accuracy: 0.7807142734527588\n",
            "Cost: 0.4658130705356598, Accuracy: 0.7817857265472412\n",
            "Cost: 0.4832991361618042, Accuracy: 0.7810714244842529\n",
            "Cost: 0.4656718373298645, Accuracy: 0.7882142663002014\n",
            "Cost: 0.4778616726398468, Accuracy: 0.7814285755157471\n",
            "Cost: 0.46534058451652527, Accuracy: 0.7846428751945496\n",
            "Cost: 0.4828931391239166, Accuracy: 0.7778571248054504\n",
            "Cost: 0.48770925402641296, Accuracy: 0.7810714244842529\n",
            "Cost: 0.47347861528396606, Accuracy: 0.7789285778999329\n",
            "Cost: 0.4774927496910095, Accuracy: 0.7903571724891663\n",
            "Cost: 0.4755709767341614, Accuracy: 0.7825000286102295\n",
            "Cost: 0.49365290999412537, Accuracy: 0.7753571271896362\n",
            "Cost: 0.4704325795173645, Accuracy: 0.7864285707473755\n",
            "Cost: 0.4688088595867157, Accuracy: 0.7889285683631897\n",
            "Cost: 0.473831444978714, Accuracy: 0.7814285755157471\n",
            "Cost: 0.47269967198371887, Accuracy: 0.7871428728103638\n",
            "Cost: 0.470419317483902, Accuracy: 0.7842857241630554\n",
            "Cost: 0.4420889616012573, Accuracy: 0.7985714077949524\n",
            "Cost: 0.4434930980205536, Accuracy: 0.7978571653366089\n",
            "Cost: 0.4406749904155731, Accuracy: 0.7946428656578064\n",
            "Cost: 0.4565684199333191, Accuracy: 0.7878571152687073\n",
            "Cost: 0.4548189043998718, Accuracy: 0.7960714101791382\n",
            "Cost: 0.4377351701259613, Accuracy: 0.8057143092155457\n",
            "Cost: 0.4429267346858978, Accuracy: 0.800000011920929\n",
            "Cost: 0.40451982617378235, Accuracy: 0.8185714483261108\n",
            "Cost: 0.4231518507003784, Accuracy: 0.8114285469055176\n",
            "Cost: 0.44089290499687195, Accuracy: 0.8010714054107666\n",
            "Cost: 0.4351743757724762, Accuracy: 0.8064285516738892\n",
            "Cost: 0.4462290406227112, Accuracy: 0.7985714077949524\n",
            "Cost: 0.427985280752182, Accuracy: 0.8028571605682373\n",
            "Cost: 0.44677263498306274, Accuracy: 0.7971428632736206\n",
            "Cost: 0.4260828495025635, Accuracy: 0.8153571486473083\n",
            "Cost: 0.4285278022289276, Accuracy: 0.8032143115997314\n",
            "Cost: 0.44695138931274414, Accuracy: 0.7960714101791382\n",
            "Cost: 0.40564489364624023, Accuracy: 0.8214285969734192\n",
            "Cost: 0.4110811948776245, Accuracy: 0.8067857027053833\n",
            "Cost: 0.41821494698524475, Accuracy: 0.8142856955528259\n",
            "Cost: 0.42181578278541565, Accuracy: 0.8050000071525574\n",
            "Cost: 0.4413132071495056, Accuracy: 0.7992857098579407\n",
            "Cost: 0.40782296657562256, Accuracy: 0.8132143020629883\n",
            "Cost: 0.4206877052783966, Accuracy: 0.8142856955528259\n",
            "Cost: 0.39915862679481506, Accuracy: 0.8253571391105652\n",
            "Cost: 0.40473276376724243, Accuracy: 0.8092857003211975\n",
            "Cost: 0.3960076868534088, Accuracy: 0.8310714364051819\n",
            "Cost: 0.3954707086086273, Accuracy: 0.8182142972946167\n",
            "Cost: 0.39865365624427795, Accuracy: 0.8264285922050476\n",
            "Cost: 0.40568533539772034, Accuracy: 0.8203571438789368\n",
            "Cost: 0.3916980028152466, Accuracy: 0.8207142949104309\n",
            "Cost: 0.3829592168331146, Accuracy: 0.8328571319580078\n",
            "Cost: 0.3944043815135956, Accuracy: 0.8257142901420593\n",
            "Cost: 0.3886319696903229, Accuracy: 0.8267857432365417\n",
            "Cost: 0.3845127522945404, Accuracy: 0.8367857336997986\n",
            "Cost: 0.3824521601200104, Accuracy: 0.8317857384681702\n",
            "Cost: 0.3786255419254303, Accuracy: 0.8246428370475769\n",
            "Cost: 0.3640757203102112, Accuracy: 0.8346428275108337\n",
            "Cost: 0.3807969391345978, Accuracy: 0.8335714340209961\n",
            "Cost: 0.36044183373451233, Accuracy: 0.8407142758369446\n",
            "Cost: 0.3601049482822418, Accuracy: 0.845714271068573\n",
            "Cost: 0.37204211950302124, Accuracy: 0.8335714340209961\n",
            "Cost: 0.3525763154029846, Accuracy: 0.8428571224212646\n",
            "Cost: 0.3718434274196625, Accuracy: 0.8339285850524902\n",
            "Cost: 0.37372827529907227, Accuracy: 0.8303571343421936\n",
            "Cost: 0.35539913177490234, Accuracy: 0.8482142686843872\n",
            "Cost: 0.3515032231807709, Accuracy: 0.841785728931427\n",
            "Cost: 0.36410975456237793, Accuracy: 0.8399999737739563\n",
            "Cost: 0.3377610445022583, Accuracy: 0.8514285683631897\n",
            "Cost: 0.3485682010650635, Accuracy: 0.8460714221000671\n",
            "Cost: 0.3382165729999542, Accuracy: 0.854285717010498\n",
            "Cost: 0.35603493452072144, Accuracy: 0.8439285755157471\n",
            "Cost: 0.3357676565647125, Accuracy: 0.8571428656578064\n",
            "Cost: 0.3437111973762512, Accuracy: 0.8514285683631897\n",
            "Cost: 0.35340797901153564, Accuracy: 0.8421428799629211\n",
            "Cost: 0.3342854380607605, Accuracy: 0.8535714149475098\n",
            "Cost: 0.31704750657081604, Accuracy: 0.8585714101791382\n",
            "Cost: 0.35204488039016724, Accuracy: 0.8424999713897705\n",
            "Cost: 0.33272987604141235, Accuracy: 0.8525000214576721\n",
            "Cost: 0.3329553008079529, Accuracy: 0.8571428656578064\n",
            "Cost: 0.35117676854133606, Accuracy: 0.852142870426178\n",
            "Cost: 0.3304295837879181, Accuracy: 0.8510714173316956\n",
            "Cost: 0.31948214769363403, Accuracy: 0.8600000143051147\n",
            "Cost: 0.32663166522979736, Accuracy: 0.858214259147644\n",
            "Cost: 0.3145148456096649, Accuracy: 0.8607142567634583\n",
            "Cost: 0.3193507790565491, Accuracy: 0.8564285635948181\n",
            "Cost: 0.3066132962703705, Accuracy: 0.871071457862854\n",
            "Cost: 0.3174314498901367, Accuracy: 0.8621428608894348\n",
            "Cost: 0.3249989151954651, Accuracy: 0.8642857074737549\n",
            "Cost: 0.3133803904056549, Accuracy: 0.864642858505249\n",
            "Cost: 0.3182778060436249, Accuracy: 0.8617857098579407\n",
            "Cost: 0.3158816993236542, Accuracy: 0.8614285588264465\n",
            "Cost: 0.3206932246685028, Accuracy: 0.8657143115997314\n",
            "Cost: 0.30678701400756836, Accuracy: 0.8735714554786682\n",
            "Cost: 0.3124431371688843, Accuracy: 0.8678571581840515\n",
            "Cost: 0.30039504170417786, Accuracy: 0.8728571534156799\n",
            "Cost: 0.31041181087493896, Accuracy: 0.8607142567634583\n",
            "Cost: 0.3032483458518982, Accuracy: 0.8650000095367432\n",
            "Cost: 0.3045046925544739, Accuracy: 0.864642858505249\n",
            "Cost: 0.3017246425151825, Accuracy: 0.8639285564422607\n",
            "Cost: 0.2763667404651642, Accuracy: 0.8760714530944824\n",
            "Cost: 0.2984437942504883, Accuracy: 0.8707143068313599\n",
            "Cost: 0.30094385147094727, Accuracy: 0.8742856979370117\n",
            "Cost: 0.2928112745285034, Accuracy: 0.871071457862854\n",
            "Cost: 0.3075801134109497, Accuracy: 0.866428554058075\n",
            "Cost: 0.2957265079021454, Accuracy: 0.8717857003211975\n",
            "Cost: 0.28266075253486633, Accuracy: 0.881428599357605\n",
            "Cost: 0.29202964901924133, Accuracy: 0.8746428489685059\n",
            "Cost: 0.2741982936859131, Accuracy: 0.8878571391105652\n",
            "Cost: 0.2806752324104309, Accuracy: 0.8782142996788025\n",
            "Cost: 0.28852376341819763, Accuracy: 0.8803571462631226\n",
            "Cost: 0.2824254631996155, Accuracy: 0.8799999952316284\n",
            "Cost: 0.2924309968948364, Accuracy: 0.8767856955528259\n",
            "Cost: 0.2706092596054077, Accuracy: 0.8832142949104309\n",
            "Cost: 0.2758804261684418, Accuracy: 0.8821428418159485\n",
            "Cost: 0.2652779817581177, Accuracy: 0.8896428346633911\n",
            "Cost: 0.2577562630176544, Accuracy: 0.8903571367263794\n",
            "Cost: 0.2689146101474762, Accuracy: 0.8824999928474426\n",
            "Cost: 0.27780723571777344, Accuracy: 0.8849999904632568\n",
            "Cost: 0.25427189469337463, Accuracy: 0.8924999833106995\n",
            "Cost: 0.2608628571033478, Accuracy: 0.8907142877578735\n",
            "Cost: 0.2674022316932678, Accuracy: 0.8885714411735535\n",
            "Cost: 0.27405431866645813, Accuracy: 0.8821428418159485\n",
            "Cost: 0.24682141840457916, Accuracy: 0.897857129573822\n",
            "Cost: 0.28371480107307434, Accuracy: 0.8824999928474426\n",
            "Cost: 0.26042288541793823, Accuracy: 0.8889285922050476\n",
            "Cost: 0.2649283707141876, Accuracy: 0.8892857432365417\n",
            "Cost: 0.25222837924957275, Accuracy: 0.8907142877578735\n",
            "Cost: 0.23850345611572266, Accuracy: 0.8992857336997986\n",
            "Cost: 0.24980610609054565, Accuracy: 0.893928587436676\n",
            "Cost: 0.24897454679012299, Accuracy: 0.893928587436676\n",
            "Cost: 0.25626692175865173, Accuracy: 0.8885714411735535\n",
            "Cost: 0.23416708409786224, Accuracy: 0.9010714292526245\n",
            "Cost: 0.2492719441652298, Accuracy: 0.8910714387893677\n",
            "Cost: 0.2514597475528717, Accuracy: 0.8914285898208618\n",
            "Cost: 0.26096582412719727, Accuracy: 0.8867856860160828\n",
            "Cost: 0.24249206483364105, Accuracy: 0.9010714292526245\n",
            "Cost: 0.23753885924816132, Accuracy: 0.9017857313156128\n",
            "Cost: 0.23841631412506104, Accuracy: 0.9003571271896362\n",
            "Cost: 0.2432142049074173, Accuracy: 0.9032142758369446\n",
            "Cost: 0.2404598742723465, Accuracy: 0.8967857360839844\n",
            "Cost: 0.2347182184457779, Accuracy: 0.8992857336997986\n",
            "Cost: 0.2208724468946457, Accuracy: 0.9067857265472412\n",
            "Cost: 0.23160262405872345, Accuracy: 0.9060714244842529\n",
            "Cost: 0.23231802880764008, Accuracy: 0.8999999761581421\n",
            "Cost: 0.23276782035827637, Accuracy: 0.9039285778999329\n",
            "Cost: 0.22435812652111053, Accuracy: 0.914642870426178\n",
            "Cost: 0.24261097609996796, Accuracy: 0.8985714316368103\n",
            "Cost: 0.2171829640865326, Accuracy: 0.9064285755157471\n",
            "Cost: 0.23571589589118958, Accuracy: 0.9017857313156128\n",
            "Cost: 0.2337971329689026, Accuracy: 0.9010714292526245\n",
            "Cost: 0.2273174673318863, Accuracy: 0.9085714221000671\n",
            "Cost: 0.21498210728168488, Accuracy: 0.9153571724891663\n",
            "Cost: 0.2206628918647766, Accuracy: 0.9075000286102295\n",
            "Cost: 0.20887228846549988, Accuracy: 0.9121428728103638\n",
            "Cost: 0.2154913693666458, Accuracy: 0.9110714197158813\n",
            "Cost: 0.23513785004615784, Accuracy: 0.9092857241630554\n",
            "Cost: 0.2173232138156891, Accuracy: 0.9139285683631897\n",
            "Cost: 0.20983494818210602, Accuracy: 0.9078571200370789\n",
            "Cost: 0.2178107500076294, Accuracy: 0.9100000262260437\n",
            "Cost: 0.22129613161087036, Accuracy: 0.9092857241630554\n",
            "Cost: 0.2122301608324051, Accuracy: 0.9107142686843872\n",
            "Cost: 0.21409283578395844, Accuracy: 0.9089285731315613\n",
            "Cost: 0.20985661447048187, Accuracy: 0.9110714197158813\n",
            "Cost: 0.20630736649036407, Accuracy: 0.9100000262260437\n",
            "Cost: 0.20892377197742462, Accuracy: 0.914642870426178\n",
            "Cost: 0.2024109959602356, Accuracy: 0.9157142639160156\n",
            "Cost: 0.1967964768409729, Accuracy: 0.9164285659790039\n",
            "Cost: 0.21107497811317444, Accuracy: 0.9128571152687073\n",
            "Cost: 0.21187040209770203, Accuracy: 0.9171428680419922\n",
            "Cost: 0.19213344156742096, Accuracy: 0.9246428608894348\n",
            "Cost: 0.20952732861042023, Accuracy: 0.9107142686843872\n",
            "Cost: 0.20299547910690308, Accuracy: 0.918571412563324\n",
            "Cost: 0.19497232139110565, Accuracy: 0.9178571701049805\n",
            "Cost: 0.198667973279953, Accuracy: 0.9164285659790039\n",
            "Cost: 0.19667628407478333, Accuracy: 0.9214285612106323\n",
            "Cost: 0.19375285506248474, Accuracy: 0.9171428680419922\n",
            "Cost: 0.19656343758106232, Accuracy: 0.9225000143051147\n",
            "Cost: 0.176271453499794, Accuracy: 0.9267857074737549\n",
            "Cost: 0.1883871853351593, Accuracy: 0.9235714077949524\n",
            "Cost: 0.1820046305656433, Accuracy: 0.9282143115997314\n",
            "Cost: 0.17921258509159088, Accuracy: 0.9307143092155457\n",
            "Cost: 0.1787789911031723, Accuracy: 0.9321428537368774\n",
            "Cost: 0.18486690521240234, Accuracy: 0.9203571677207947\n",
            "Cost: 0.18714237213134766, Accuracy: 0.9242857098579407\n",
            "Cost: 0.1939455270767212, Accuracy: 0.920714259147644\n",
            "Cost: 0.16275730729103088, Accuracy: 0.9353571534156799\n",
            "Cost: 0.18814130127429962, Accuracy: 0.9239285588264465\n",
            "Cost: 0.17624209821224213, Accuracy: 0.927142858505249\n",
            "Cost: 0.17523051798343658, Accuracy: 0.920714259147644\n",
            "Cost: 0.18417781591415405, Accuracy: 0.9225000143051147\n",
            "Cost: 0.17610815167427063, Accuracy: 0.927142858505249\n",
            "Cost: 0.16399984061717987, Accuracy: 0.9389285445213318\n",
            "Cost: 0.17490871250629425, Accuracy: 0.9321428537368774\n",
            "Cost: 0.1573946326971054, Accuracy: 0.9367856979370117\n",
            "Cost: 0.16762115061283112, Accuracy: 0.9325000047683716\n",
            "Cost: 0.17275451123714447, Accuracy: 0.9257143139839172\n",
            "Cost: 0.17019681632518768, Accuracy: 0.9353571534156799\n",
            "Cost: 0.162993922829628, Accuracy: 0.9350000023841858\n",
            "Cost: 0.1503259241580963, Accuracy: 0.9417856931686401\n",
            "Cost: 0.15996696054935455, Accuracy: 0.933571457862854\n",
            "Cost: 0.16145242750644684, Accuracy: 0.9357143044471741\n",
            "Cost: 0.1764892339706421, Accuracy: 0.9300000071525574\n",
            "Cost: 0.17382127046585083, Accuracy: 0.9278571605682373\n",
            "Cost: 0.15398405492305756, Accuracy: 0.9392856955528259\n",
            "Cost: 0.17253009974956512, Accuracy: 0.9328571557998657\n",
            "Cost: 0.16361628472805023, Accuracy: 0.9357143044471741\n",
            "Cost: 0.1530304104089737, Accuracy: 0.9375\n",
            "Cost: 0.15995271503925323, Accuracy: 0.9392856955528259\n",
            "Cost: 0.15811030566692352, Accuracy: 0.9371428489685059\n",
            "Cost: 0.16737879812717438, Accuracy: 0.9350000023841858\n",
            "Cost: 0.15085773169994354, Accuracy: 0.9421428442001343\n",
            "Cost: 0.1582842618227005, Accuracy: 0.9353571534156799\n",
            "Cost: 0.15458254516124725, Accuracy: 0.946071445941925\n",
            "Cost: 0.13954973220825195, Accuracy: 0.9528571367263794\n",
            "Cost: 0.142292320728302, Accuracy: 0.9474999904632568\n",
            "Cost: 0.1464705914258957, Accuracy: 0.9410714507102966\n",
            "Cost: 0.1428225189447403, Accuracy: 0.9453571438789368\n",
            "Cost: 0.14488708972930908, Accuracy: 0.9435714483261108\n",
            "Cost: 0.14778965711593628, Accuracy: 0.9382143020629883\n",
            "Cost: 0.14230960607528687, Accuracy: 0.9496428370475769\n",
            "Cost: 0.13678039610385895, Accuracy: 0.9482142925262451\n",
            "Cost: 0.1351839303970337, Accuracy: 0.9442856907844543\n",
            "Cost: 0.13874675333499908, Accuracy: 0.9514285922050476\n",
            "Cost: 0.13255999982357025, Accuracy: 0.9485714435577393\n",
            "Cost: 0.14821623265743256, Accuracy: 0.9378571510314941\n",
            "Cost: 0.12660010159015656, Accuracy: 0.9510714411735535\n",
            "Cost: 0.14024695754051208, Accuracy: 0.941428542137146\n",
            "Cost: 0.13010002672672272, Accuracy: 0.9510714411735535\n",
            "Cost: 0.12182455509901047, Accuracy: 0.9585714340209961\n",
            "Cost: 0.12775057554244995, Accuracy: 0.956428587436676\n",
            "Cost: 0.15720036625862122, Accuracy: 0.9371428489685059\n",
            "Cost: 0.1347976177930832, Accuracy: 0.9449999928474426\n",
            "Cost: 0.1248127743601799, Accuracy: 0.9535714387893677\n",
            "Cost: 0.13766822218894958, Accuracy: 0.9464285969734192\n",
            "Cost: 0.13480505347251892, Accuracy: 0.9453571438789368\n",
            "Cost: 0.11800882965326309, Accuracy: 0.9567857384681702\n",
            "Cost: 0.11613719165325165, Accuracy: 0.9642857313156128\n",
            "Cost: 0.13006313145160675, Accuracy: 0.9517857432365417\n",
            "Cost: 0.12365411221981049, Accuracy: 0.9507142901420593\n",
            "Cost: 0.13281676173210144, Accuracy: 0.9485714435577393\n",
            "Cost: 0.13396845757961273, Accuracy: 0.946071445941925\n",
            "Cost: 0.11244860291481018, Accuracy: 0.9574999809265137\n",
            "Cost: 0.12123160064220428, Accuracy: 0.9546428322792053\n",
            "Cost: 0.12379869818687439, Accuracy: 0.9471428394317627\n",
            "Cost: 0.13193538784980774, Accuracy: 0.9489285945892334\n",
            "Cost: 0.12115544080734253, Accuracy: 0.9524999856948853\n",
            "Cost: 0.11533798277378082, Accuracy: 0.9599999785423279\n",
            "Cost: 0.12677855789661407, Accuracy: 0.9528571367263794\n",
            "Cost: 0.1280040740966797, Accuracy: 0.9485714435577393\n",
            "Cost: 0.11582166701555252, Accuracy: 0.9592857360839844\n",
            "Cost: 0.1199391707777977, Accuracy: 0.9510714411735535\n",
            "Cost: 0.11221951991319656, Accuracy: 0.9539285898208618\n",
            "Cost: 0.1183965727686882, Accuracy: 0.9553571343421936\n",
            "Cost: 0.10744818300008774, Accuracy: 0.9657142758369446\n",
            "Cost: 0.11182545125484467, Accuracy: 0.9617857336997986\n",
            "Cost: 0.10596293956041336, Accuracy: 0.9607142806053162\n",
            "Cost: 0.11282341927289963, Accuracy: 0.9589285850524902\n",
            "Cost: 0.124665267765522, Accuracy: 0.9503571391105652\n",
            "Cost: 0.11550508439540863, Accuracy: 0.956428587436676\n",
            "Cost: 0.11080245673656464, Accuracy: 0.9589285850524902\n",
            "Cost: 0.1170576810836792, Accuracy: 0.954285740852356\n",
            "Cost: 0.11194135248661041, Accuracy: 0.9632142782211304\n",
            "Cost: 0.10624709725379944, Accuracy: 0.9592857360839844\n",
            "Cost: 0.10883434861898422, Accuracy: 0.9553571343421936\n",
            "Cost: 0.10159864276647568, Accuracy: 0.9653571248054504\n",
            "Cost: 0.09842608124017715, Accuracy: 0.9678571224212646\n",
            "Cost: 0.10641778260469437, Accuracy: 0.9599999785423279\n",
            "Cost: 0.09604675322771072, Accuracy: 0.9682142734527588\n",
            "Cost: 0.10412056744098663, Accuracy: 0.9649999737739563\n",
            "Cost: 0.10676973313093185, Accuracy: 0.9639285802841187\n",
            "Cost: 0.11799715459346771, Accuracy: 0.9549999833106995\n",
            "Cost: 0.09408888965845108, Accuracy: 0.9678571224212646\n",
            "Cost: 0.10753271728754044, Accuracy: 0.9635714292526245\n",
            "Cost: 0.09367126226425171, Accuracy: 0.966785728931427\n",
            "Cost: 0.10224206000566483, Accuracy: 0.9599999785423279\n",
            "Cost: 0.08927533775568008, Accuracy: 0.9685714244842529\n",
            "Cost: 0.09677048027515411, Accuracy: 0.966785728931427\n",
            "Cost: 0.10931405425071716, Accuracy: 0.9571428298950195\n",
            "Cost: 0.100814089179039, Accuracy: 0.9635714292526245\n",
            "Cost: 0.09508224576711655, Accuracy: 0.9682142734527588\n",
            "Cost: 0.09417953342199326, Accuracy: 0.9674999713897705\n",
            "Cost: 0.09555608779191971, Accuracy: 0.9642857313156128\n",
            "Cost: 0.0919293463230133, Accuracy: 0.970714271068573\n",
            "Cost: 0.09840533882379532, Accuracy: 0.9671428799629211\n",
            "Cost: 0.09083575755357742, Accuracy: 0.9674999713897705\n",
            "Cost: 0.10097856819629669, Accuracy: 0.9610714316368103\n",
            "Cost: 0.09327488392591476, Accuracy: 0.9642857313156128\n",
            "Cost: 0.08191806077957153, Accuracy: 0.9696428775787354\n",
            "Cost: 0.09353131055831909, Accuracy: 0.9617857336997986\n",
            "Cost: 0.08896905928850174, Accuracy: 0.9710714221000671\n",
            "Cost: 0.0992128774523735, Accuracy: 0.9653571248054504\n",
            "Cost: 0.10113716870546341, Accuracy: 0.9639285802841187\n",
            "Cost: 0.09778045862913132, Accuracy: 0.9657142758369446\n",
            "Cost: 0.09037886559963226, Accuracy: 0.9696428775787354\n",
            "Cost: 0.09182604402303696, Accuracy: 0.9664285778999329\n",
            "Cost: 0.08991634100675583, Accuracy: 0.9696428775787354\n",
            "Cost: 0.08763109147548676, Accuracy: 0.9703571200370789\n",
            "Cost: 0.09514294564723969, Accuracy: 0.9632142782211304\n",
            "Cost: 0.09126751124858856, Accuracy: 0.9664285778999329\n",
            "Cost: 0.08587121218442917, Accuracy: 0.9674999713897705\n",
            "Cost: 0.08223798125982285, Accuracy: 0.9678571224212646\n",
            "Cost: 0.07827069610357285, Accuracy: 0.9760714173316956\n",
            "Cost: 0.079909548163414, Accuracy: 0.9742857217788696\n",
            "Cost: 0.09203885495662689, Accuracy: 0.9660714268684387\n",
            "Cost: 0.07812662422657013, Accuracy: 0.9742857217788696\n",
            "Cost: 0.08358684182167053, Accuracy: 0.9717857241630554\n",
            "Cost: 0.08092501014471054, Accuracy: 0.9714285731315613\n",
            "Cost: 0.07891526073217392, Accuracy: 0.9735714197158813\n",
            "Cost: 0.07060988992452621, Accuracy: 0.9789285659790039\n",
            "Cost: 0.08415433764457703, Accuracy: 0.9703571200370789\n",
            "Cost: 0.08310000598430634, Accuracy: 0.9674999713897705\n",
            "Cost: 0.07743723690509796, Accuracy: 0.9678571224212646\n",
            "Cost: 0.07963977009057999, Accuracy: 0.9732142686843872\n",
            "Cost: 0.08241989463567734, Accuracy: 0.9710714221000671\n",
            "Cost: 0.08641931414604187, Accuracy: 0.9714285731315613\n",
            "Cost: 0.08326710015535355, Accuracy: 0.9689285755157471\n",
            "Cost: 0.07555757462978363, Accuracy: 0.9742857217788696\n",
            "Cost: 0.08137894421815872, Accuracy: 0.9692857265472412\n",
            "Cost: 0.07289665192365646, Accuracy: 0.9760714173316956\n",
            "Cost: 0.08187007158994675, Accuracy: 0.9714285731315613\n",
            "Cost: 0.07092444598674774, Accuracy: 0.9778571724891663\n",
            "Cost: 0.07455374300479889, Accuracy: 0.9739285707473755\n",
            "Cost: 0.08072910457849503, Accuracy: 0.9689285755157471\n",
            "Cost: 0.07627759873867035, Accuracy: 0.9717857241630554\n",
            "Cost: 0.07448505610227585, Accuracy: 0.9728571176528931\n",
            "Cost: 0.07121232151985168, Accuracy: 0.977142870426178\n",
            "Cost: 0.06557530909776688, Accuracy: 0.9796428680419922\n",
            "Cost: 0.07539615035057068, Accuracy: 0.9767857193946838\n",
            "Cost: 0.073472760617733, Accuracy: 0.9739285707473755\n",
            "Cost: 0.06782668828964233, Accuracy: 0.977142870426178\n",
            "Cost: 0.08025161921977997, Accuracy: 0.9721428751945496\n",
            "Cost: 0.06929197162389755, Accuracy: 0.9782142639160156\n",
            "Cost: 0.07219012826681137, Accuracy: 0.9750000238418579\n",
            "Cost: 0.08665918558835983, Accuracy: 0.9717857241630554\n",
            "Cost: 0.06852395087480545, Accuracy: 0.977142870426178\n",
            "Cost: 0.06405872106552124, Accuracy: 0.9778571724891663\n",
            "Cost: 0.0650998055934906, Accuracy: 0.9789285659790039\n",
            "Cost: 0.08155246078968048, Accuracy: 0.9696428775787354\n",
            "Cost: 0.07080162316560745, Accuracy: 0.9753571152687073\n",
            "Cost: 0.07415072619915009, Accuracy: 0.9721428751945496\n",
            "Cost: 0.06389744579792023, Accuracy: 0.9775000214576721\n",
            "Cost: 0.062371838837862015, Accuracy: 0.9814285635948181\n",
            "Cost: 0.06710966676473618, Accuracy: 0.9760714173316956\n",
            "Cost: 0.06393804401159286, Accuracy: 0.9778571724891663\n",
            "Cost: 0.06400652974843979, Accuracy: 0.9796428680419922\n",
            "Cost: 0.0639246329665184, Accuracy: 0.9778571724891663\n",
            "Cost: 0.06250713765621185, Accuracy: 0.9789285659790039\n",
            "Cost: 0.0708921030163765, Accuracy: 0.9785714149475098\n",
            "Cost: 0.05820407345890999, Accuracy: 0.9817857146263123\n",
            "Cost: 0.06105180084705353, Accuracy: 0.9803571701049805\n",
            "Cost: 0.0585143156349659, Accuracy: 0.9803571701049805\n",
            "Cost: 0.06259486079216003, Accuracy: 0.9785714149475098\n",
            "Cost: 0.06243131309747696, Accuracy: 0.9807142615318298\n",
            "Cost: 0.06603137403726578, Accuracy: 0.9778571724891663\n",
            "Cost: 0.056439708918333054, Accuracy: 0.9821428656578064\n",
            "Cost: 0.06121693179011345, Accuracy: 0.979285717010498\n",
            "Cost: 0.06379483640193939, Accuracy: 0.9800000190734863\n",
            "Cost: 0.06450187414884567, Accuracy: 0.9775000214576721\n",
            "Cost: 0.05906203016638756, Accuracy: 0.9796428680419922\n",
            "Cost: 0.0611732043325901, Accuracy: 0.981071412563324\n",
            "Cost: 0.05477319285273552, Accuracy: 0.9835714101791382\n",
            "Cost: 0.05477540194988251, Accuracy: 0.9828571677207947\n",
            "Cost: 0.05553356185555458, Accuracy: 0.981071412563324\n",
            "Cost: 0.05130309984087944, Accuracy: 0.9842857122421265\n",
            "Cost: 0.05847706273198128, Accuracy: 0.9846428632736206\n",
            "Cost: 0.06443626433610916, Accuracy: 0.9757142663002014\n",
            "Cost: 0.052906256169080734, Accuracy: 0.983214259147644\n",
            "Cost: 0.0530293807387352, Accuracy: 0.983214259147644\n",
            "Cost: 0.059597309678792953, Accuracy: 0.9807142615318298\n",
            "Cost: 0.0554644875228405, Accuracy: 0.9825000166893005\n",
            "Cost: 0.060484129935503006, Accuracy: 0.9821428656578064\n",
            "Cost: 0.06425834447145462, Accuracy: 0.979285717010498\n",
            "Cost: 0.058884747326374054, Accuracy: 0.9796428680419922\n",
            "Cost: 0.05256472900509834, Accuracy: 0.9821428656578064\n",
            "Cost: 0.05463586002588272, Accuracy: 0.9803571701049805\n",
            "Cost: 0.056678544729948044, Accuracy: 0.9814285635948181\n",
            "Cost: 0.05357974395155907, Accuracy: 0.9850000143051147\n",
            "Cost: 0.05531344935297966, Accuracy: 0.9803571701049805\n",
            "Cost: 0.05528629198670387, Accuracy: 0.9835714101791382\n",
            "Cost: 0.04899166524410248, Accuracy: 0.9850000143051147\n",
            "Cost: 0.0513308010995388, Accuracy: 0.9842857122421265\n",
            "Cost: 0.05331556126475334, Accuracy: 0.9825000166893005\n",
            "Cost: 0.05853592976927757, Accuracy: 0.9796428680419922\n",
            "Cost: 0.04960561543703079, Accuracy: 0.9857142567634583\n",
            "Cost: 0.043516576290130615, Accuracy: 0.987500011920929\n",
            "Cost: 0.046528108417987823, Accuracy: 0.9864285588264465\n",
            "Cost: 0.04574170708656311, Accuracy: 0.9846428632736206\n",
            "Cost: 0.05586504936218262, Accuracy: 0.9817857146263123\n",
            "Cost: 0.049441125243902206, Accuracy: 0.9839285612106323\n",
            "Cost: 0.04779591038823128, Accuracy: 0.9867857098579407\n",
            "Cost: 0.04821612685918808, Accuracy: 0.9864285588264465\n",
            "Cost: 0.055329907685518265, Accuracy: 0.9846428632736206\n",
            "Cost: 0.05178666114807129, Accuracy: 0.9828571677207947\n",
            "Cost: 0.05540623143315315, Accuracy: 0.9803571701049805\n",
            "Cost: 0.0485115610063076, Accuracy: 0.983214259147644\n",
            "Cost: 0.050938960164785385, Accuracy: 0.9842857122421265\n",
            "Cost: 0.04937940463423729, Accuracy: 0.9839285612106323\n",
            "Cost: 0.04962165653705597, Accuracy: 0.9853571653366089\n",
            "Cost: 0.048878468573093414, Accuracy: 0.983214259147644\n",
            "Cost: 0.04887695237994194, Accuracy: 0.983214259147644\n",
            "Cost: 0.042703136801719666, Accuracy: 0.9878571629524231\n",
            "Cost: 0.041440147906541824, Accuracy: 0.989642858505249\n",
            "Cost: 0.04234195128083229, Accuracy: 0.9892857074737549\n",
            "Cost: 0.05149652808904648, Accuracy: 0.9853571653366089\n",
            "Cost: 0.04388057067990303, Accuracy: 0.9864285588264465\n",
            "Cost: 0.04867934063076973, Accuracy: 0.9853571653366089\n",
            "0.81333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}